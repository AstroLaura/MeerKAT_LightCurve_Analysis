{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"figure.figsize\": (12,9),\n",
    "          \"font.size\": 20,\n",
    "          \"font.weight\": \"normal\",\n",
    "          \"xtick.major.size\": 9,\n",
    "          \"xtick.minor.size\": 4,\n",
    "          \"ytick.major.size\": 9,\n",
    "          \"ytick.minor.size\": 4,\n",
    "          \"xtick.major.width\": 4,\n",
    "          \"xtick.minor.width\": 3,\n",
    "          \"ytick.major.width\": 4,\n",
    "          \"ytick.minor.width\": 3,\n",
    "          \"xtick.major.pad\": 8,\n",
    "          \"xtick.minor.pad\": 8,\n",
    "          \"ytick.major.pad\": 8,\n",
    "          \"ytick.minor.pad\": 8,\n",
    "          \"lines.linewidth\": 3,\n",
    "          \"lines.markersize\": 10,\n",
    "          \"axes.linewidth\": 4,\n",
    "          \"legend.loc\": \"best\",\n",
    "          \"text.usetex\": False,    \n",
    "          \"xtick.labelsize\" : 20,\n",
    "          \"ytick.labelsize\" : 20,\n",
    "          }\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "from astropy import units as un\n",
    "from astropy.coordinates import SkyCoord\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as spstats\n",
    "\n",
    "from matplotlib.offsetbox import AnchoredText\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def madulation_parameter(data, dud_epoch=False):\n",
    "    '''\n",
    "    Use the median and MAD to make a modulation-type parameter\n",
    "    \n",
    "    This parameter takes a 1-D numpy array and calculates\n",
    "    the MAD (median absolute deviation) parameter.\n",
    "    We first calculate the median and mad\n",
    "    for the data. Then we subtract the mean from each value\n",
    "    and divide by the MAD. The final MAD parameter is the\n",
    "    absolute value of the extremus.\n",
    "    \n",
    "    Args:\n",
    "    data (array): a 1-D numpy array of values\n",
    "    \n",
    "    kwargs:\n",
    "    dud_epoch (int/array): an integer or array of integers\n",
    "                           giving the indices of data values\n",
    "                           to remove/ignore\n",
    "                           Default: False (no values to\n",
    "                           remove/ignore)\n",
    "                           \n",
    "    returns:\n",
    "    (madvalue, madlocation)\n",
    "    madvalue is the value of the MAD parameter for these\n",
    "    data. madlocation is the index of the epoch where the\n",
    "    mad parameter is from.\n",
    "    '''\n",
    "    # Remove nans as they affect the calculation\n",
    "    data_bob = np.copy(data[np.where(~np.isnan(data))[0]])\n",
    "    me = np.nanmedian(data_bob)\n",
    "    # Use the unscaled MAD (not the equivalent of the\n",
    "    # standard deviation)\n",
    "    ma = spstats.median_absolute_deviation(data_bob,\n",
    "                                           scale=1.0,\n",
    "                                           nan_policy='omit')\n",
    "    \n",
    "    # Remove dodgey epochs if needed\n",
    "    data_ = np.copy(data_bob)\n",
    "    if dud_epoch:\n",
    "        data_[dud_epoch] = np.nan\n",
    "    \n",
    "    # Subtract the median from all values\n",
    "    shifted_data = np.abs(data_ - me)\n",
    "    # divide the subtracted values\n",
    "    # by the MAD\n",
    "    divided_by_mad = shifted_data/ma\n",
    "    \n",
    "    # Find the maximum value\n",
    "    try:\n",
    "        madvalue = np.nanmax(divided_by_mad)\n",
    "        madlocation = np.nanargmax(divided_by_mad)\n",
    "    except ValueError:\n",
    "        # just in case all the values are\n",
    "        # nans in the data\n",
    "        madvalue = np.nan\n",
    "        madlocation = np.nan\n",
    "    \n",
    "    return madvalue, madlocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi2_parameter(data, uncertainties):\n",
    "    '''\n",
    "    Calculate the chi2 variability parameter for a set of points.\n",
    "    \n",
    "    A description of the variability parameter\n",
    "    can be found here:\n",
    "    https://tkp.readthedocs.io/en/r3.0/devref/\n",
    "    database/schema.html#appendices\n",
    "    \n",
    "    Args:\n",
    "    data (array): a 1D row array of data points\n",
    "    uncertainties (array): the 1D array of uncertainties\n",
    "                           that corresponds to the data array\n",
    "    \n",
    "    Returns:\n",
    "    A float that is the value of the variability\n",
    "    parameter for the data array\n",
    "    '''\n",
    "    weights = 1./(uncertainties**2.)\n",
    "    \n",
    "    p1 = len(data[~np.isnan(data)])/(len(data[~np.isnan(data)])-1.)\n",
    "    p2 = np.nanmean(weights*(data**2.))\n",
    "    p3 = ((np.nanmean(weights*data))**2.)/(np.nanmean(weights))\n",
    "    \n",
    "    return p1 * (p2 - p3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modulation_parameter(data):\n",
    "    '''\n",
    "    Calculate the modulation parameter for a set of points.\n",
    "    \n",
    "    A description of the modulation parameter\n",
    "    can be found here:\n",
    "    https://tkp.readthedocs.io/en/r3.0/devref/\n",
    "    database/schema.html#appendices\n",
    "    \n",
    "    Args:\n",
    "    data (array): a 1D row array of data points\n",
    "    \n",
    "    Returns:\n",
    "    A float that is the value of the modulation\n",
    "    parameter for the data array\n",
    "    '''\n",
    "    p1 = 1./np.nanmean(data)\n",
    "    p2 = len(data)/(len(data)-1)\n",
    "    p3 = np.nanmean(data**2.) - (np.nanmean(data))**2.\n",
    "    \n",
    "    return p1 * np.sqrt(p2*p3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_extended(lcfs, extended_file,\n",
    "                    ra_col='ra',\n",
    "                    dec_col='decl'):\n",
    "    '''\n",
    "    Filters sources from a list of source files\n",
    "    \n",
    "    This takes the file names of light curve files\n",
    "    and outputs a list of sources that *don't*\n",
    "    match the coordinates of the extended_file\n",
    "    sources.\n",
    "    \n",
    "    Args:\n",
    "    lcfs (list): list of file names (including the\n",
    "                 path) of light curve files\n",
    "    extended_file (str): the name (including the\n",
    "                         path) of the numpy file\n",
    "                         containing the list of source\n",
    "                         coordinates that you want to\n",
    "                         filter out\n",
    "    kwargs:\n",
    "    ra_col (str): The name of the right ascension column\n",
    "                  Default = 'ra'\n",
    "    dec_col (str): The name of the declination column\n",
    "                   Default = 'decl'\n",
    "\n",
    "    Returns:\n",
    "    A list of filenames of sources that *don't*\n",
    "    match the coordinates in extended_file\n",
    "    '''\n",
    "    extended_sources = np.load(extended_file)\n",
    "    es_sc = SkyCoord(extended_sources, unit=(un.deg, un.deg))\n",
    "    \n",
    "    lcs_cut = []\n",
    "    for s, source in enumerate(lcfs):\n",
    "        lc = pd.read_csv(source)\n",
    "\n",
    "        lc_coord = SkyCoord(np.array([[np.nanmean(lc[ra_col]),\n",
    "                                       np.nanmean(lc[dec_col])]]),\n",
    "                            unit=(un.degree, un.degree))\n",
    "\n",
    "        seps = lc_coord.separation(es_sc)\n",
    "        min_sep = np.nanmin(seps.deg)\n",
    "\n",
    "        if min_sep > 3./60./60.:\n",
    "            lcs_cut.append(source)\n",
    "\n",
    "    return lcs_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_sn(lcfs, min_sn,\n",
    "              flux_col='f_int',\n",
    "              flux_err_col='f_int_err'):\n",
    "    '''\n",
    "    Remove light curves with S/N less than min_sn\n",
    "    \n",
    "    This function removes any sources that have\n",
    "    a signal to noise less than min_sn.\n",
    "    The fluxes of every source are divided by the\n",
    "    uncertainties to give the approximate\n",
    "    signal to noise. If there is no detection greater\n",
    "    than zero, or if there is no epoch where the source\n",
    "    has a signal to noise greater than (default) 2, then\n",
    "    the source is excluded from the analysis.\n",
    "    \n",
    "    Args:\n",
    "    lcfs (list): list of file names (including the\n",
    "                 path) of light curve files\n",
    "    min_sn (float): the minimum allowed signal\n",
    "                    to noise\n",
    "\n",
    "    Kwargs:\n",
    "    flux_col (str): The name of the flux column\n",
    "                    Default ='f_int'\n",
    "    flux_err_col (str): The name of the flux error column\n",
    "                        Default ='f_int_err'\n",
    "    Returns:\n",
    "    A list of filenames of sources that have\n",
    "    a signal to noise over min_sn\n",
    "    '''    \n",
    "    lcs_cut = []\n",
    "    for s, source in enumerate(lcfs):\n",
    "        lc = pd.read_csv(source)\n",
    "\n",
    "        # Get an approximation of the\n",
    "        # signal to noise for each epoch\n",
    "        signal_to_noise = lc[flux_col] / lc[flux_err_col]\n",
    "        \n",
    "        min_val = lc[flux_col] - lc[flux_err_col]\n",
    "\n",
    "        # If the source has at least one detection\n",
    "        # greater than the sinal to noise limit,\n",
    "        # include it in the analysis\n",
    "        if ((np.nanmax(signal_to_noise) > min_sn) and\n",
    "            (np.nanmax(min_val) > 0.)):\n",
    "            lcs_cut.append(source)\n",
    "    return lcs_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dpc(lcfs,\n",
    "               distance_from_phase_centre,\n",
    "               pc_col='dist_to_pc_DEG'):\n",
    "    '''\n",
    "    Filters sources that are\n",
    "    too distance from the phase centre\n",
    "    \n",
    "    This takes the file names of light curve files\n",
    "    and outputs a list of sources that are within\n",
    "    distance_from_phase_centre from the phase\n",
    "    centre\n",
    "    \n",
    "    Args:\n",
    "    lcfs (list): list of file names (including the\n",
    "                 path) of light curve files\n",
    "    distance_from_phase_centre (float): the maximum allowed\n",
    "                                        distance from the\n",
    "                                        phase centre that\n",
    "                                        a source can be\n",
    "                                        to be included\n",
    "                                        (in degrees)\n",
    "    kwargs:\n",
    "    pc_col (str): The name of the distance from phase\n",
    "                  centre column\n",
    "                  Default = 'dist_to_pc_DEG'\n",
    "\n",
    "    Returns:\n",
    "    A list of filenames of sources that are less than the\n",
    "    maximum distance from the phase centre\n",
    "    '''\n",
    "    lc_cut = []\n",
    "    for l, lcf in enumerate(lcfs):\n",
    "        # Read the light curve file\n",
    "        lc = pd.read_csv(lcf)\n",
    "        # Make sure the information\n",
    "        # is in chronological order\n",
    "        lc = lc.sort_values(by=[mjd_col])\n",
    "\n",
    "        pc_sep = np.nanmean(lc[pc_col])\n",
    "        if pc_sep <= distance_from_phase_centre:\n",
    "            lc_cut.append(lcf)\n",
    "\n",
    "    return lc_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_coords(lcfs, coords,\n",
    "                  ra_col='ra',\n",
    "                  dec_col='decl',\n",
    "                  sep=3./60./60.):\n",
    "    '''\n",
    "    Filters sources from a list of coordinates\n",
    "    \n",
    "    This takes the file names of light curve files\n",
    "    and outputs a list of sources that *don't*\n",
    "    match the coordinates of coords.\n",
    "    \n",
    "    Args:\n",
    "    lcfs (list): list of file names (including the\n",
    "                 path) of light curve files\n",
    "    coords (SkyCoord array): array of astropy SkyCoords\n",
    "                             of sources you want to\n",
    "                             filter out\n",
    "    kwargs:\n",
    "    ra_col (str): The name of the right ascension column\n",
    "                  Default = 'ra'\n",
    "    dec_col (str): The name of the declination column\n",
    "                   Default = 'decl'\n",
    "    sep (float): if two sources are less than sep (in degrees)\n",
    "                 apart they are considered to match\n",
    "\n",
    "    Returns:\n",
    "    A list of filenames of sources that *don't*\n",
    "    match the coordinates in coords\n",
    "    '''\n",
    "    lc_cut = []\n",
    "    for l, lcf in enumerate(lcfs):\n",
    "        # Read the light curve file\n",
    "        lc = pd.read_csv(lcf)\n",
    "        # Turn the source coordinates into an\n",
    "        # astropy SkyCoord object\n",
    "        lc_coord = SkyCoord(np.array([[np.nanmean(lc[ra_col]),\n",
    "                                       np.nanmean(lc[dec_col])]]),\n",
    "                            unit=(un.degree, un.degree))\n",
    "        # Check if the source is one of the\n",
    "        # known variables and move on if\n",
    "        # it is one\n",
    "        seps = lc_coord.separation(coords)\n",
    "        if np.nanmin(seps.deg)<sep:\n",
    "            print('Source {} is a known variable'.format(l))\n",
    "        else:\n",
    "            lc_cut.append(lcf)\n",
    "\n",
    "    return lc_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_epochs(lcs,\n",
    "                      mjd_col='mjd'):\n",
    "    '''\n",
    "    Get all of the unique MJDs from all files\n",
    "    \n",
    "    This gets the unique MJDs from a list\n",
    "    of csv files (in pandas format)\n",
    "    \n",
    "    Args:\n",
    "    lcs (list): list of file names (including\n",
    "                the path) of the pandas\n",
    "                light curve files\n",
    "    kwargs:\n",
    "    mjd_col (str): the name of the column\n",
    "                   containing the MJDs in the\n",
    "                   files\n",
    "                   Default: 'mjd'\n",
    "    Returns:\n",
    "    An array containing the unique MJD\n",
    "    values from all the light curve files\n",
    "    '''\n",
    "    all_mjds = []\n",
    "    for lc in lcs:\n",
    "        mjd = pd.read_csv(lcs[0])[mjd_col]\n",
    "        all_mjds += list(mjd)\n",
    "    mjds = np.unique(np.array(all_mjds))\n",
    "\n",
    "    return mjds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_fluxes(lcfs,\n",
    "                 ref_epoch='None',\n",
    "                 flux_col='f_int',\n",
    "                 ra_col='ra',\n",
    "                 dec_col='decl',\n",
    "                 mjd_col='mjd'):\n",
    "    '''\n",
    "    Scale all the fluxes by the flux from one epoch\n",
    "    \n",
    "    To find the systematics we need to divide all\n",
    "    of the fluxes for each source by the flux of the\n",
    "    reference epoch. This function does that.\n",
    "    You can either set the reference epoch yourself,\n",
    "    or it will choose the epoch with the most\n",
    "    values in it (i.e. where the highest number of\n",
    "    sources are detected)\n",
    "    It also ignores sources that don't have any\n",
    "    values with a signal to noise of greater\n",
    "    than sig_noise.\n",
    "    \n",
    "    Args:\n",
    "    lcfs (list): a list of strings, where each string is the\n",
    "                 path to a pandas csv. Each pandas csv has the\n",
    "                 light curve and information for a single source\n",
    "    \n",
    "    kwargs:\n",
    "    ref_epoch (int): The index of the reference\n",
    "                     epoch. If 'None' an epoch will\n",
    "                     be chosen. The selected epoch\n",
    "                     will have the most detections.\n",
    "                     Default = 'None'\n",
    "    flux_col (str): The name of the flux column\n",
    "                    Default ='f_int'\n",
    "    ra_col (str): The name of the riht ascension column\n",
    "                  Default = 'ra'\n",
    "    dec_col (str): The name of the declination column\n",
    "                   Default = 'decl'\n",
    "    mjd_col (str): The name of the mjd column\n",
    "                   Default = 'mjd'\n",
    "                      \n",
    "    Returns:\n",
    "    An array of scaled fluxes where each row is\n",
    "    a different sources, and the columns are the epochs\n",
    "    (in chronological order)\n",
    "    '''\n",
    "    # Find out how many epochs you expect\n",
    "    # per source\n",
    "    mjds = get_unique_epochs(lcfs)\n",
    "    num_mjds = len(mjds)\n",
    "    \n",
    "    all_fluxes = []\n",
    "    for l, lcf in enumerate(lcfs):\n",
    "        # Read the light curve file\n",
    "        lc = pd.read_csv(lcf)\n",
    "        # Make sure the information\n",
    "        # is in chronological order\n",
    "        lc = lc.sort_values(by=[mjd_col])\n",
    "        \n",
    "        # Check if the source has the right number of\n",
    "        # MJDs (they all should, but it's a useful check)\n",
    "        if len(lc[flux_col]) == num_mjds:\n",
    "            all_fluxes.append(list(np.array(lc[flux_col])))\n",
    "        else:\n",
    "            print(('Wrong number of mjds? '\n",
    "                   'Should be {0}, is {1}').format(num_mjds,\n",
    "                                                   len(lc[flux_col])))\n",
    "            print('lc with wrong number of mjds:\\n{}'.format(lcf))\n",
    "            pass\n",
    "\n",
    "    # Turn the list of source fluxes into an array\n",
    "    all_fluxes = np.array(all_fluxes)\n",
    "    \n",
    "    # If no reference epoch is specified, work\n",
    "    # out which epoch has the highest number\n",
    "    # of detected sources and use that\n",
    "    if ref_epoch == 'None':\n",
    "        num_nans = np.zeros(len(all_fluxes[0]))\n",
    "        for c, col in enumerate(all_fluxes.T):\n",
    "            nans = np.where(np.isnan(col))\n",
    "            num_nans[c] = len(nans[0])\n",
    "        ref_epoch = np.nanargmin(num_nans)\n",
    "    print('Reference epoch is epoch {}'.format(ref_epoch))\n",
    "    \n",
    "    # Divide the light curves of every source\n",
    "    # by the flux of the reference epoch for\n",
    "    # each source (i.e. each source is divided\n",
    "    # by it's own reference epoch flux)\n",
    "    bob = all_fluxes[:, ref_epoch]\n",
    "    bob = np.expand_dims(bob, axis=0)\n",
    "    bob = bob.T\n",
    "    \n",
    "    # Return the array of source flux\n",
    "    # densities that have been scaled\n",
    "    # to the reference epoch\n",
    "    return all_fluxes / bob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epoch_offsets(scaled_fluxes,\n",
    "                      mjds,\n",
    "                      image_folder,\n",
    "                      database,\n",
    "                      dataset_id,\n",
    "                      ext,\n",
    "                      outlier_sigmas=3.0,\n",
    "                      pc_cut='None',\n",
    "                      close_all=True):\n",
    "    '''\n",
    "    Work out the light curve systematics\n",
    "    \n",
    "    This function takes the flux density array\n",
    "    from scale_fluxes and uses it to find out\n",
    "    the systematic offset for each epoch.\n",
    "    Specifically, it takes the distribution\n",
    "    of scaled fluxes for each epoch and finds\n",
    "    the mean, standard deviation, median,\n",
    "    and median absolute deviation and returns\n",
    "    a pandas dataframe with thse values and the\n",
    "    corresponding mjds\n",
    "    \n",
    "    Args:\n",
    "    scaled_fluxes (array): the array of scaled fluxes\n",
    "                           made by the scale_fluxes function\n",
    "    mjds (array): The mjds for each epoch in an array\n",
    "    image_folder (str): This function makes and couple\n",
    "                        of plots, this is a string of the\n",
    "                        path where you'd like to save\n",
    "                        those plots\n",
    "    database (str): The name of the TraP database, this will\n",
    "                    be used in the plot filenames\n",
    "    dataset_id (int): the number of the TraP dataset_id, this\n",
    "                      be used in the plot filenames\n",
    "    ext (str): added extension at the end of file names\n",
    "               (for both csvs and pngs)\n",
    "    \n",
    "    kwargs:\n",
    "    outlier_sigmas (float): The number of sigma away source\n",
    "                            has to be from the mean to be removed\n",
    "                            from the calculation of the mean\n",
    "                            and standard deviation. This\n",
    "                            is the avoid skewing the analysis\n",
    "                            by real variable/transient sources\n",
    "                            Default=3.0\n",
    "    close_all (bool): if True all the plots will be closed\n",
    "                      Default: True\n",
    "    Returns:\n",
    "    A pandas data frame with:\n",
    "    'mu': mean per epoch, 'std': standard deviation per epoch,\n",
    "    'median': median per epoch, 'mad': median absolute deviation per epoch,\n",
    "    'mjd': mjds\n",
    "\n",
    "    The median absolute deviation (MAD) is produced by\n",
    "    scipy.stats.median_absolute_deviation which includes\n",
    "    a default scale of 1.4826 to ensure \"consistency with\n",
    "    the standard deviation for normally distributed data.\"\n",
    "    '''\n",
    "    # Set up an empty array to put the mean, std,\n",
    "    # median and MAD\n",
    "    models = np.zeros((len(scaled_fluxes[0]), 4))\n",
    "\n",
    "    # Go through by epoch\n",
    "    for column, col in enumerate(scaled_fluxes[0]):\n",
    "        fluxes = scaled_fluxes[:, column]\n",
    "        # Remove nans as the fitting won't allow them\n",
    "        fluxes = fluxes[np.logical_not(np.isnan(scaled_fluxes[:,\n",
    "                                                              column]))]\n",
    "        # Get the median scaled flux for this epoch\n",
    "        median = np.nanmedian(fluxes)\n",
    "        # Get the MAD for the scaled flux for this epoch\n",
    "        # Making sure to divide by the sqrt(n) to make it\n",
    "        # the population MAD\n",
    "        mad = (spstats.median_absolute_deviation(fluxes,\n",
    "                                                 nan_policy='omit')/\n",
    "               np.sqrt(len(fluxes)))\n",
    "\n",
    "        # Remove initial outliers as these are likely variable\n",
    "        # and will have a strong affect on the mean and std\n",
    "        pre_mu, pre_std = spstats.norm.fit(fluxes)\n",
    "        plus3sig = np.where(fluxes>(pre_mu + outlier_sigmas*pre_std))[0]\n",
    "        minus3sig = np.where(fluxes<(pre_mu - outlier_sigmas*pre_std))[0]\n",
    "        outliers = np.concatenate((plus3sig, minus3sig))\n",
    "        fluxes_prepped = np.delete(fluxes, outliers, axis=0)\n",
    "\n",
    "        # Check that this epoch has measurements and\n",
    "        # isn't just nans\n",
    "        if len(fluxes_prepped) == 0:\n",
    "            print('***********************************')\n",
    "            print('No sources for column: {}'.format(column))\n",
    "            print('***********************************')\n",
    "            mu = np.nan\n",
    "            std = np.nan\n",
    "        else:\n",
    "            # Get the mean and standard deviation\n",
    "            mu = np.nanmean(fluxes_prepped)\n",
    "            std = np.nanstd(fluxes_prepped)\n",
    "            num_not_nan = len(fluxes_prepped)\n",
    "            # Make sure you use the population\n",
    "            # standard deviation (divide by sqrt n)\n",
    "            pop_std = std / np.sqrt(num_not_nan)\n",
    "\n",
    "        # Put the mean, std, median, and mad for this epoch\n",
    "        # in the prepared blank array\n",
    "        models[column] = [mu, pop_std, median, mad]\n",
    "\n",
    "        # Plot the Gaussian for the epoch and fit results\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "\n",
    "        a = ax.hist(fluxes_prepped,\n",
    "                    bins=20,\n",
    "                    density=True,\n",
    "                    facecolor='LightBlue')\n",
    "\n",
    "        xmin, xmax = ax.get_xlim()\n",
    "        ymin, ymax = ax.get_ylim()\n",
    "        x = np.linspace(xmin, xmax, 100)\n",
    "        y = np.linspace(ymin, ymax, 10)\n",
    "\n",
    "        ax.plot(np.ones(10)*mu, y, '--',\n",
    "                c='DarkGrey')\n",
    "\n",
    "        p = spstats.norm.pdf(x, mu, std)\n",
    "        ax.plot(x, p, linewidth=2, c='RoyalBlue')\n",
    "\n",
    "        ax.set_ylabel('Count', fontsize=18)\n",
    "        ax.set_xlabel('Epoch {}: flux/ref_flux'.format(column),\n",
    "                      fontsize=18)\n",
    "\n",
    "        ax.set_xlim(xmin, xmax)\n",
    "        ax.set_ylim(ymin, ymax)\n",
    "\n",
    "        atext = AnchoredText(('Mean: {0:.4f}\\n'\n",
    "                              'StD: {1:.4f}\\n'\n",
    "                              'Median: {2:.4f}\\n'\n",
    "                              'MAD: {3:.4f}\\n').format(mu,\n",
    "                                                       std,\n",
    "                                                       median,\n",
    "                                                       mad),\n",
    "                             loc='upper right',\n",
    "                             prop=dict(fontsize=16))\n",
    "        ax.add_artist(atext)\n",
    "\n",
    "        if column < 10:\n",
    "            epoch_str = '00{}'.format(column)\n",
    "        elif column < 100:\n",
    "            epoch_str = '0{}'.format(column)\n",
    "        else:\n",
    "            epoch_str = str(column)\n",
    "\n",
    "        if pc_cut == 'None':\n",
    "            fig.savefig(('{0}EpochGaussian_Epoch{1}_'\n",
    "                         'db{2}_ds{3}_{4}.png').format(image_folder,\n",
    "                                                   epoch_str,\n",
    "                                                   database,\n",
    "                                                   dataset_id, ext),\n",
    "                        bbox_inches='tight')\n",
    "        else:\n",
    "            fig.savefig(('{0}EpochGaussian_Epoch{1}_'\n",
    "                         'db{2}_ds{3}_'\n",
    "                         'pccut{4:.3f}deg_{5}.png').format(image_folder,\n",
    "                                                       epoch_str,\n",
    "                                                       database,\n",
    "                                                       dataset_id, pc_cut, ext),\n",
    "                        bbox_inches='tight')\n",
    "        if close_all:\n",
    "            plt.close(fig)\n",
    "        else:\n",
    "            if column % 10 != 0:\n",
    "                plt.close(fig)\n",
    "\n",
    "    # Plot the means and medians for every epoch\n",
    "    # These are the models for the\n",
    "    # systematics\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(12, 12),\n",
    "                           sharex=True, sharey=True)\n",
    "\n",
    "    ax[0].errorbar(mjds, models[:, 0],\n",
    "                yerr=models[:, 1],\n",
    "                fmt='o', c='#ff87ab',\n",
    "                label='Mean and std', elinewidth=5, ms=10)\n",
    "    ax[1].errorbar(mjds, models[:, 2],\n",
    "                yerr=models[:, 3],\n",
    "                fmt='o', c='#6D98BA', label='Median and mad')\n",
    "\n",
    "    xmin, xmax = ax[0].get_xlim()\n",
    "    ymin, ymax = ax[0].get_ylim()\n",
    "\n",
    "    xs = np.linspace(xmin, xmax, 10)\n",
    "    for a in range(2):\n",
    "        ax[a].plot(xs, np.ones(10), '--', c='DarkGrey', zorder=0)\n",
    "\n",
    "        ax[a].set_xlim(xmin, xmax)\n",
    "        ax[a].set_ylim(ymin, ymax)\n",
    "        ax[a].legend()\n",
    "    ax[0].set_ylabel('Epoch mean', fontsize=18)\n",
    "    ax[1].set_ylabel('Epoch median', fontsize=18)\n",
    "    ax[1].set_xlabel('MJD', fontsize=18)\n",
    "\n",
    "    if pc_cut == 'None':\n",
    "        fig.savefig(('{0}EpochGaussian_AllEpochs_'\n",
    "                     'db{2}_ds{3}_{4}.png').format(image_folder,\n",
    "                                               column,\n",
    "                                               database,\n",
    "                                               dataset_id, ext),\n",
    "                    bbox_inches='tight')\n",
    "    else:\n",
    "        fig.savefig(('{0}EpochGaussian_AllEpochs_'\n",
    "                     'db{2}_ds{3}_'\n",
    "                     'pccut{4:.3f}deg_{5}.png').format(image_folder,\n",
    "                                                   column,\n",
    "                                                   database,\n",
    "                                                   dataset_id,\n",
    "                                                   pc_cut, ext),\n",
    "                    bbox_inches='tight')\n",
    "    if close_all:\n",
    "        plt.close(fig)\n",
    "\n",
    "    # Return the values as a pandas dataframe\n",
    "    df_data = {'mu': models[:, 0],\n",
    "               'std': models[:, 1],\n",
    "               'median': models[:, 2],\n",
    "               'mad': models[:, 3],\n",
    "               'mjd': mjds}\n",
    "    return pd.DataFrame(data=df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_lightcurves(lcfs, scale_models,\n",
    "                      database, dataset_id,\n",
    "                      ref_epoch, csv_savepath,\n",
    "                      ext,\n",
    "                      flux_col = 'f_int',\n",
    "                      mjd_col='mjd',\n",
    "                      ra_col='ra',\n",
    "                      dec_col='decl',\n",
    "                      pc_cut='None'):\n",
    "    '''\n",
    "    Apply the scale models to all light curves\n",
    "    \n",
    "    This function applies the scale models found\n",
    "    using get_epoch_offsets. It applies both\n",
    "    the median and mean model, putting them into\n",
    "    different columns. It also saves the models\n",
    "    themselves in the tables for each light curve.\n",
    "    The final light curve files will have\n",
    "    the new columns:\n",
    "    'ref scale model mean': the systematics model\n",
    "                            made from the mean of\n",
    "                            each epoch\n",
    "    'ref scale model std': the population standard\n",
    "                           devation uncertanties\n",
    "                           for the mean scale model\n",
    "    'ref scale model median': the systematics model\n",
    "                              made from the median of\n",
    "                              each epoch\n",
    "    'ref scale model mad': the population MAD uncertainties\n",
    "                           for the median scale model\n",
    "    flux_col+'_mean_scaled': the source fluxes scaled\n",
    "                             using the mean model (Jy)\n",
    "    flux_col+'_median_scaled': the source fluxes scaled\n",
    "                               using the median model (Jy)\n",
    "    flux_col+'_mean_scaled_err': the uncertainties on the\n",
    "                                 source fluxes scaled by\n",
    "                                 the std (mean model)\n",
    "    flux_col+'_median_scaled_err': the uncertainties on the\n",
    "                                   source fluxes scaled by\n",
    "                                   the MAD (median model)\n",
    "    'mean_scaled_V': V parameter of the mean model scaled fluxes\n",
    "    'median_scaled_V': V parameter of the median model scaled fluxes\n",
    "    'mean_scaled_eta': eta parameter of the mean model scaled fluxes\n",
    "    'median_scaled_eta': eta parameter of the median model scaled fluxes\n",
    "    'mean_scaled_madp': MAD parameter of the mean model scaled fluxes\n",
    "    'median_scaled_madp': MAD parameter of the median model scaled fluxes\n",
    "    The best way to read and use the light curve files is using pandas.\n",
    "    \n",
    "    Args:\n",
    "    lcfs: a list of the light curve/source info files (including paths)\n",
    "    scale_models: the scale model pandas date frame from\n",
    "                  get_epoch_offsets\n",
    "    database: the name of the TraP database\n",
    "    dataset_id: the name of the TraP dataset_id\n",
    "    ref_epoch: the epoch used as the reference epoch\n",
    "    csv_savepath: where you want to save the final, updated\n",
    "                  light curve/source info csvs\n",
    "    close_all (bool): if True all the plots will be closed\n",
    "                      Default: True\n",
    "    \n",
    "    kwargs:\n",
    "    flux_col (str): The name of the flux column\n",
    "                    Default ='f_int'\n",
    "    ra_col (str): The name of the riht ascension column\n",
    "                  Default = 'ra'\n",
    "    dec_col (str): The name of the declination column\n",
    "                   Default = 'decl'\n",
    "    mjd_col (str): The name of the mjd column\n",
    "                   Default = 'mjd'\n",
    "                   \n",
    "    Returns:\n",
    "    a list of the new light curves info pandas frames\n",
    "    (i.e. one panda frame per source)\n",
    "    '''\n",
    "    # Make an empty list to put the updated\n",
    "    # light curve data frames for later use\n",
    "    new_lcs = []\n",
    "\n",
    "    # Loop through each source file\n",
    "    for l, lcf in enumerate(lcfs):\n",
    "        lc = pd.read_csv(lcf)\n",
    "        # Make sure the light curve is in\n",
    "        # chronological order\n",
    "        lc = lc.sort_values(by=[mjd_col])\n",
    "\n",
    "        # Add columns to include the models as they are\n",
    "        lc['ref scale model mean'] = scale_models['mu']\n",
    "        lc['ref scale model std'] = scale_models['std']\n",
    "        lc['ref scale model median'] = scale_models['median']\n",
    "        lc['ref scale model mad'] = scale_models['mad']\n",
    "        # Scale the flux densities by the scale models\n",
    "        # and put the values in new columns so that you\n",
    "        # can also keep the original fluxes\n",
    "        lc[flux_col+'_mean_scaled'] = lc[flux_col] * (1./scale_models['mu'])\n",
    "        lc[flux_col+'_median_scaled'] = lc[flux_col] * (1./scale_models['median'])\n",
    "\n",
    "        # Scale the uncertainties and put those in new columns too\n",
    "        new_flux_uncertainty_mean = (lc[flux_col+'_mean_scaled'] *\n",
    "                                ((lc['{}_err'.format(flux_col)]/lc[flux_col]) +\n",
    "                                 (scale_models['std']/scale_models['mu'])))\n",
    "        lc[flux_col+'_mean_scaled_err'] = new_flux_uncertainty_mean\n",
    "        \n",
    "        new_flux_uncertainty_median = (lc[flux_col+'_median_scaled'] *\n",
    "                                       ((lc['{}_err'.format(flux_col)]/lc[flux_col]) +\n",
    "                                        (scale_models['mad']/scale_models['median'])))\n",
    "        lc[flux_col+'_median_scaled_err'] = new_flux_uncertainty_median\n",
    "\n",
    "        # When you calculate the new/scaled variability\n",
    "        # parameters, you have to ditch the reference\n",
    "        # epoch, because that will skew your variability\n",
    "        # parameters. So here we make arrays where the\n",
    "        # reference epoch has been removeed, so we can\n",
    "        # calculate the new variability parameters\n",
    "        allowed_fluxes_mean = np.array(lc[flux_col+'_mean_scaled'])\n",
    "        allowed_fluxes_mean = np.delete(allowed_fluxes_mean,\n",
    "                                        [int(ref_epoch)], 0)\n",
    "        allowed_fluxes_mean_errs = np.array(lc[flux_col+'_mean_scaled_err'])\n",
    "        allowed_fluxes_mean_errs = np.delete(allowed_fluxes_mean_errs,\n",
    "                                             [int(ref_epoch)], 0)\n",
    "        \n",
    "        allowed_fluxes_median = np.array(lc[flux_col+'_median_scaled'])\n",
    "        allowed_fluxes_median = np.delete(allowed_fluxes_median,\n",
    "                                          [int(ref_epoch)], 0)\n",
    "        allowed_fluxes_median_errs = np.array(lc[flux_col+'_median_scaled_err'])\n",
    "        allowed_fluxes_median_errs = np.delete(allowed_fluxes_median_errs,\n",
    "                                               [int(ref_epoch)], 0)\n",
    "\n",
    "        # Now calculate the new variability parameters for\n",
    "        # both the median and mean scaled fluxes\n",
    "        Vs_mean = np.repeat(modulation_parameter(allowed_fluxes_mean),\n",
    "                            len(lc))\n",
    "        lc['mean_scaled_V'] = Vs_mean                            \n",
    "        Vs_median = np.repeat(modulation_parameter(allowed_fluxes_median),\n",
    "                              len(lc))\n",
    "        lc['median_scaled_V'] = Vs_median\n",
    "\n",
    "        try:\n",
    "            etas_mean = np.repeat(chi2_parameter(allowed_fluxes_mean,\n",
    "                                                 allowed_fluxes_mean_errs),\n",
    "                                  len(lc))\n",
    "        except ZeroDivisionError:\n",
    "            print('source: ', l, 'etas error')\n",
    "            etas_mean = np.repeat(np.nan, len(lc))\n",
    "        lc['mean_scaled_eta'] = etas_mean\n",
    "\n",
    "        try:\n",
    "            etas_median = np.repeat(chi2_parameter(allowed_fluxes_median,\n",
    "                                                   allowed_fluxes_median_errs),\n",
    "                                    len(lc))\n",
    "        except ZeroDivisionError:\n",
    "            print('source: ', l, 'etas error')\n",
    "            etas_median = np.repeat(np.nan, len(lc))\n",
    "        lc['median_scaled_eta'] = etas_median\n",
    "\n",
    "        madps_mean = np.repeat(madulation_parameter(allowed_fluxes_mean)[0], len(lc))\n",
    "        lc['mean_scaled_madp'] = madps_mean\n",
    "        madps_median = np.repeat(madulation_parameter(allowed_fluxes_median)[0], len(lc))\n",
    "        lc['median_scaled_madp'] = madps_median\n",
    "\n",
    "        # Put the light curve into the prepared list,\n",
    "        # and save the data as a csv\n",
    "        new_lcs.append(lc)\n",
    "        if pc_cut == 'None':\n",
    "            savename = ('{0}rcat{1}_'\n",
    "                        'ra{2:.5f}_dec{3:.5f}_'\n",
    "                        'db{4}_ds{5}_'\n",
    "                        '{6}.csv').format(csv_savepath,\n",
    "                                         lc['runcat'][0],\n",
    "                                         np.nanmean(lc[ra_col]),\n",
    "                                         np.nanmean(lc[dec_col]),\n",
    "                                         database, dataset_id, ext)\n",
    "            \n",
    "        else:\n",
    "            savename = ('{0}rcat{1}_'\n",
    "                        'ra{2:.5f}_dec{3:.5f}_'\n",
    "                        'db{4}_ds{5}_'\n",
    "                        'pccut{6:.3f}deg_'\n",
    "                        '{7}.csv').format(csv_savepath,\n",
    "                                         lc['runcat'][0],\n",
    "                                         np.nanmean(lc[ra_col]),\n",
    "                                         np.nanmean(lc[dec_col]),\n",
    "                                         database, dataset_id,\n",
    "                                         pc_cut, ext)\n",
    "        lc.to_csv(savename)\n",
    "\n",
    "    return new_lcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_variability_parameters(new_lcs, csv_savepath,\n",
    "                               database,\n",
    "                               dataset_id,\n",
    "                               ext,\n",
    "                               pc_cut='None',\n",
    "                               min_sn=2):\n",
    "    '''\n",
    "    Calculate the new variability parameters\n",
    "    for the scaled light curves\n",
    "    \n",
    "    Get the eta, V and MAD variability parameters\n",
    "    for the scaled light curves (both median and mean\n",
    "    scaled)\n",
    "    \n",
    "    Args:\n",
    "    new_lcs (list): list of filenames (including the path)\n",
    "                    of the scaled light curve files\n",
    "    database (str): the name of the TraP database\n",
    "    dataset_id (int): the dataset number from TraP\n",
    "    \n",
    "    kwargs:\n",
    "    pc_cut (float): the distance from the phase centre\n",
    "                    in degrees within which sources\n",
    "                    will be included in the analysis\n",
    "                    Default: 'None' (all sources included)\n",
    "    min_sn (float): the minimum allowed signal\n",
    "                    to noise\n",
    "                    Default: 2\n",
    "    \n",
    "    Returns:\n",
    "    A pandas dataframe with the new and old variability\n",
    "    parameters for each source\n",
    "    The dataframe is also saved as a csv\n",
    "    '''\n",
    "    variability_parameters = np.zeros((len(new_lcs), 14))\n",
    "\n",
    "    source_detected_2 = []\n",
    "    source_detected_3 = []\n",
    "    for nl, nlc in enumerate(new_lcs):\n",
    "        flux = nlc['f_int']\n",
    "        flux_errs = nlc['f_int_err']\n",
    "\n",
    "        # Work out whether the source is\n",
    "        # \"detected\" to include this in the\n",
    "        # dataframe\n",
    "        signal_to_noise = flux / flux_errs\n",
    "        if (np.nanmax(signal_to_noise) > 3):\n",
    "            source_detected_3.append('Yes')\n",
    "            source_detected_2.append('Yes')\n",
    "        elif(np.nanmax(signal_to_noise) > 2):\n",
    "            source_detected_2.append('Yes')\n",
    "            source_detected_3.append('No')\n",
    "        else:\n",
    "            source_detected_2.append('No')\n",
    "            source_detected_3.append('No')\n",
    "\n",
    "        variability_parameters[nl] = [nlc['runcat'][0], nlc['freq_eff_int'][0],\n",
    "                                      np.nanmedian(nlc['f_int']),\n",
    "                                      np.nanmedian(nlc['f_int_mean_scaled']),\n",
    "                                      np.nanmedian(nlc['f_int_median_scaled']),\n",
    "                                      nlc['V_param'][0],\n",
    "                                      nlc['mean_scaled_V'][0],\n",
    "                                      nlc['median_scaled_V'][0],\n",
    "                                      nlc['eta_param'][0],\n",
    "                                      nlc['mean_scaled_eta'][0],\n",
    "                                      nlc['median_scaled_eta'][0],\n",
    "                                      nlc['mad_param'][0],\n",
    "                                      nlc['mean_scaled_madp'][0],\n",
    "                                      nlc['median_scaled_madp'][0]]\n",
    "\n",
    "    # Set up the data to turn into a dataframe\n",
    "    data_dict = {'runcat': variability_parameters[:, 0],\n",
    "                 'freq_eff_int': variability_parameters[:, 1],\n",
    "                 'f_int_median': variability_parameters[:, 2],\n",
    "                 'f_int_mean_scaled_median': variability_parameters[:, 3],\n",
    "                 'f_int_median_scaled_median': variability_parameters[:, 4],\n",
    "                 'V_param': variability_parameters[:, 5],\n",
    "                 'mean_scaled_V': variability_parameters[:, 6],\n",
    "                 'median_scaled_V': variability_parameters[:, 7],\n",
    "                 'eta_param': variability_parameters[:, 8],\n",
    "                 'mean_scaled_eta': variability_parameters[:, 9],\n",
    "                 'median_scaled_eta': variability_parameters[:, 10],\n",
    "                 'mad_param': variability_parameters[:, 11],\n",
    "                 'mean_scaled_madp': variability_parameters[:, 12],\n",
    "                 'median_scaled_madp': variability_parameters[:, 13],\n",
    "                 'detected SN2':source_detected_2,\n",
    "                 'detected SN3':source_detected_3}\n",
    "\n",
    "    # Save the data frame\n",
    "    var_param_frame = pd.DataFrame(data=data_dict)\n",
    "    if pc_cut == 'None':\n",
    "        var_param_frame.to_csv('{0}VariabilityParams_db{1}_'\n",
    "                               'ds{2}_{3}.csv'.format(csv_savepath,\n",
    "                                                      database,\n",
    "                                                      dataset_id, ext))\n",
    "    else:\n",
    "        var_param_frame.to_csv(('{0}VariabilityParams_'\n",
    "                                'db{1}_ds{2}_'\n",
    "                                'pccut{3:.3f}deg_'\n",
    "                                '{4}.csv').format(csv_savepath,\n",
    "                                                  database,\n",
    "                                                  dataset_id,\n",
    "                                                  pc_cut, ext))\n",
    "    return var_param_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_var_params(var_param_frame,\n",
    "                    image_folder, \n",
    "                    database,\n",
    "                    dataset_id,\n",
    "                    ext,\n",
    "                    pc_cut='None',\n",
    "                    close_all=True):\n",
    "    '''\n",
    "    Plot the variability parameters\n",
    "    \n",
    "    This plots the variability parameters\n",
    "    before and after correction\n",
    "    \n",
    "    Args:\n",
    "    var_param_frame (pandas dataframe): the pandas dataframe\n",
    "                                        containing the variability\n",
    "                                        parameter information\n",
    "    image_folder (str): the path to where you'd like to save\n",
    "                        the plot\n",
    "    database (str): the name of the TraP database\n",
    "    dataset_id (int): the dataset number from TraP\n",
    "    \n",
    "    kwargs:\n",
    "    pc_cut (float): the distance from the phase centre\n",
    "                    in degrees within which sources\n",
    "                    will be included in the analysis\n",
    "                    Default: 'None' (all sources included)\n",
    "    '''\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(15, 5), sharex=True)\n",
    "\n",
    "    ax[0].scatter(np.log10(var_param_frame['f_int_median']),\n",
    "                  np.log10(var_param_frame['eta_param']),\n",
    "                  marker='.', c='DarkGrey',\n",
    "                  label='Original')\n",
    "    ax[0].scatter(np.log10(var_param_frame['f_int_mean_scaled_median']),\n",
    "                  np.log10(var_param_frame['mean_scaled_eta']),\n",
    "                  marker='.', c='#FF87AB',\n",
    "                  label='Scaled', s=200)\n",
    "\n",
    "    ax[1].scatter(np.log10(var_param_frame['f_int_median']),\n",
    "                  np.log10(var_param_frame['V_param']),\n",
    "                  marker='.', c='DarkGrey',\n",
    "                  label='Original')\n",
    "    ax[1].scatter(np.log10(var_param_frame['f_int_mean_scaled_median']),\n",
    "                  np.log10(var_param_frame['mean_scaled_V']),\n",
    "                  marker='.', c='#FF87AB',\n",
    "                  label='Scaled', s=200)\n",
    "\n",
    "    ax[2].scatter(np.log10(var_param_frame['f_int_median']),\n",
    "                  np.log10(var_param_frame['mad_param']),\n",
    "                  marker='.', c='DarkGrey',\n",
    "                  label='Original')\n",
    "    ax[2].scatter(np.log10(var_param_frame['f_int_mean_scaled_median']),\n",
    "                  np.log10(var_param_frame['mean_scaled_madp']),\n",
    "                  marker='.', c='#FF87AB',\n",
    "                  label='Mean scaled', s=200)\n",
    "\n",
    "    ax[0].scatter(np.log10(var_param_frame['f_int_median_scaled_median']),\n",
    "                  np.log10(var_param_frame['median_scaled_eta']),\n",
    "                  marker='.', c='#6D98BA',\n",
    "                  label='Median scaled', s=80)\n",
    "\n",
    "    ax[1].scatter(np.log10(var_param_frame['f_int_median_scaled_median']),\n",
    "                  np.log10(var_param_frame['median_scaled_V']),\n",
    "                  marker='.', c='#6D98BA',\n",
    "                  label='Median scaled', s=80)\n",
    "\n",
    "    ax[2].scatter(np.log10(var_param_frame['f_int_median_scaled_median']),\n",
    "                  np.log10(var_param_frame['median_scaled_madp']),\n",
    "                  marker='.', c='#6D98BA',\n",
    "                  label='Median scaled', s=80)\n",
    "\n",
    "    for a in range(3):\n",
    "        ax[a].set_xlabel(r'log$_{10}\\left(\\mathrm{integrated\\,flux\\,[Jy]} \\right)$', fontsize=16)\n",
    "    ax[0].set_ylabel(r'log$_{10}\\left(\\eta \\right)$', fontsize=16)\n",
    "    ax[1].set_ylabel(r'log$_{10}\\left(V \\right)$', fontsize=16)\n",
    "    ax[2].set_ylabel(r'log$_{10}\\left(MAD \\right)$', fontsize=16)\n",
    "\n",
    "    ax[2].legend(fontsize=12, loc='best')\n",
    "    \n",
    "    if close_all:\n",
    "        plt.close(fig)\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    if pc_cut == 'None':\n",
    "        fig.savefig(('{0}EpochGaussian_VariabilityParams_'\n",
    "                     '_db{1}_ds{2}_{3}.png').format(image_folder, \n",
    "                                                database,\n",
    "                                                dataset_id, ext),\n",
    "                    bbox_inches='tight')\n",
    "    else:\n",
    "        fig.savefig(('{0}EpochGaussian_VariabilityParams_'\n",
    "                     '_db{1}_ds{2}_'\n",
    "                     'pccut{3:.3f}deg_{4}.png').format(image_folder,\n",
    "                                                   database,\n",
    "                                                   dataset_id, pc_cut),\n",
    "                    bbox_inches='tight')\n",
    "    \n",
    "    if close_all:\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name of the TraP database\n",
    "database = 'Laura_NWLayers_ReProc'\n",
    "\n",
    "# Paths to light curve files\n",
    "lc_path = ('/raid/driessen/FlareStars/'\n",
    "           'GX339/Source_Light_Curves/')\n",
    "# Path to where to save the plots\n",
    "image_folder = ('/raid/driessen/FlareStars/'\n",
    "                'GX339/EpochScaling/')\n",
    "# Path to where to save the CSV files\n",
    "csv_savepath = ('/raid/driessen/FlareStars/'\n",
    "                'GX339/Source_Light_Curves/'\n",
    "                'Average_Scaled/')\n",
    "\n",
    "# Path to the file containing the\n",
    "# coordinates of known resolved\n",
    "# sources and artefacts\n",
    "extended_sources = ('/raid/driessen/FlareStars/'\n",
    "                    'GX339/2021.01.18_GX339_'\n",
    "                    'KnownExtendedSources.npy')\n",
    "\n",
    "# Column names for light curve files\n",
    "flux_col = 'f_int'\n",
    "flux_err_col = 'f_int_err'\n",
    "mjd_col = 'mjd'\n",
    "ra_col = 'ra'\n",
    "dec_col = 'decl'\n",
    "freq_col = 'freq_eff_int'\n",
    "\n",
    "# Minimum required signal\n",
    "# to noise\n",
    "min_signaltonoise = 3.\n",
    "\n",
    "# Dataset IDs for TraP and their corresponding\n",
    "# subband frequencies\n",
    "dataset_ids = [49, 52, 53, 50, 54, 55, 51, 33]\n",
    "freqs = [1658, 1551, 1444, 1337, 1123, 1016, 909, 'MFS']\n",
    "\n",
    "# Primary beam sizes\n",
    "# (Not used here unless\n",
    "# code is edited)\n",
    "pbs = []\n",
    "for freq in freqs[:-1]:\n",
    "    pbs.append(np.rad2deg(np.arcsin((1.22*(3e8/(freq*1.0e6)))/13.9))/2.)\n",
    "\n",
    "# Coordinates of known variables\n",
    "# MKT J170456.2-482100, PSR J1703-4851,\n",
    "# and GX 339-4\n",
    "known_variables = SkyCoord(np.array([[256.23450507,\n",
    "                                      -48.35008655],\n",
    "                                     [255.97732915,\n",
    "                                      -48.86685808],\n",
    "                                     [255.70567674,\n",
    "                                      -48.78963475]]), unit=(un.degree,\n",
    "                                                             un.degree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for d, dataset_id in enumerate(dataset_ids):\n",
    "    for cut_extended in [True, False]:\n",
    "        # Get the light curve files that are from\n",
    "        # TraP\n",
    "        lcs_original = glob.glob(('{0}rcat*_'\n",
    "                                  'ra*_dec*_'\n",
    "                                  'db{1}_ds{2}_'\n",
    "                                  '*Source.csv').format(lc_path,\n",
    "                                                        database,\n",
    "                                                        dataset_id))\n",
    "\n",
    "        freq = pd.read_csv(lcs_original[1])[freq_col][0]\n",
    "        print('*******************')\n",
    "        print('Freq: {}'.format(freq))\n",
    "        print('*******************')\n",
    "\n",
    "        print('Total number of light curves: {}'.format(len(lcs_original)))\n",
    "\n",
    "        # Choose your reference epoch\n",
    "        mjds = get_unique_epochs(lcs_original)\n",
    "        print('Number of epochs: {}'.format(len(mjds)))\n",
    "        ref_epoch = len(mjds) - 1\n",
    "        print('Reference epoch: {}'.format(ref_epoch))\n",
    "\n",
    "        # Remove the extended sources and artefacts\n",
    "        if cut_extended:\n",
    "            lcs = filter_extended(lcs_original,\n",
    "                                  extended_sources)\n",
    "            ext = 'PS'\n",
    "            print('*** Point sources only ***', ext)\n",
    "        else:\n",
    "            lcs = lcs_original\n",
    "            ext = 'ES'\n",
    "            print('*** All sources ***', ext)\n",
    "        print(('Number of light curves after '\n",
    "               'extended cut: {}').format(len(lcs)))\n",
    "\n",
    "        # Cut sources below a specified signal to noise\n",
    "        lcs_cut_sn = filter_sn(lcs, min_signaltonoise)\n",
    "        print(('Number of light curves after '\n",
    "               'S/N cut: {}').format(len(lcs_cut_sn)))\n",
    "\n",
    "        # Remove known variable sources\n",
    "        lcs_cut_vars = filter_coords(lcs_cut_sn, known_variables)\n",
    "        print(('Number of light curves after '\n",
    "               'known variable cut: {}').format(len(lcs_cut_vars)))\n",
    "\n",
    "        # Scale the light curves by the flux density of\n",
    "        # the refence epoch\n",
    "        scaled_fluxes = scale_fluxes(lcs_cut_vars,\n",
    "                                     ref_epoch=ref_epoch)\n",
    "        print('Scaled fluxes: ', np.shape(scaled_fluxes))\n",
    "\n",
    "        # Determine the scale models\n",
    "        scale_models = get_epoch_offsets(scaled_fluxes,\n",
    "                                         mjds,\n",
    "                                         image_folder,\n",
    "                                         database,\n",
    "                                         dataset_id,\n",
    "                                         ext)\n",
    "\n",
    "        # Scale each light curve by the models\n",
    "        new_lcs = scale_lightcurves(lcs, scale_models,\n",
    "                                    database, dataset_id,\n",
    "                                    ref_epoch, csv_savepath,\n",
    "                                    ext)\n",
    "\n",
    "        # Make some data frames containing the variability\n",
    "        # parameter information of each source (this is\n",
    "        # useful for plottin etc.)\n",
    "        var_param_frame = get_variability_parameters(new_lcs, csv_savepath,\n",
    "                                                     database,\n",
    "                                                     dataset_id,\n",
    "                                                     ext,\n",
    "                                                     min_sn=2.)\n",
    "\n",
    "        # Plot the variability parameters\n",
    "        plot_var_params(var_param_frame,\n",
    "                        image_folder, \n",
    "                        database,\n",
    "                        dataset_id,\n",
    "                        ext,\n",
    "                        close_all=True)\n",
    "\n",
    "        # Make a plot of the models so that you can\n",
    "        # check that nothing has gone obviously\n",
    "        # wrong\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "        ax.errorbar(scale_models['mjd'], scale_models['median'],\n",
    "                    yerr=scale_models['mad'],\n",
    "                    fmt='.', c='HotPink', label='Median and MAD')\n",
    "        ax.errorbar(scale_models['mjd'], scale_models['mu'],\n",
    "                    yerr=scale_models['std'],\n",
    "                    fmt='.', c='Pink', label='Mean and STD')\n",
    "\n",
    "        ax.legend()\n",
    "        ax.set_xlabel('MJD', fontsize=18)\n",
    "        ax.set_ylabel('Model', fontsize=18)\n",
    "        ax.set_title('{0}MHz {1}'.format(freq, ext))\n",
    "        fig.tight_layout()\n",
    "\n",
    "        print('----------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
