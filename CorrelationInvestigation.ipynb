{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "import scipy as sp\n",
    "import scipy.stats as spstats\n",
    "\n",
    "from astropy import units as un\n",
    "from astropy.coordinates import SkyCoord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_extended(lcfs, extended_file,\n",
    "                    ra_col='ra',\n",
    "                    dec_col='decl'):\n",
    "    '''\n",
    "    Filters sources from a list of source files\n",
    "    \n",
    "    This takes the file names of light curve files\n",
    "    and outputs a list of sources that *don't*\n",
    "    match the coordinates of the extended_file\n",
    "    sources.\n",
    "    \n",
    "    Args:\n",
    "    lcfs (list): list of file names (including the\n",
    "                 path) of light curve files\n",
    "    extended_file (str): the name (including the\n",
    "                         path) of the numpy file\n",
    "                         containing the list of source\n",
    "                         coordinates that you want to\n",
    "                         filter out\n",
    "    kwargs:\n",
    "    ra_col (str): The name of the right ascension column\n",
    "                  Default = 'ra'\n",
    "    dec_col (str): The name of the declination column\n",
    "                   Default = 'decl'\n",
    "\n",
    "    Returns:\n",
    "    A list of filenames of sources that *don't*\n",
    "    match the coordinates in extended_file\n",
    "    '''\n",
    "    extended_sources = np.load(extended_file)\n",
    "    es_sc = SkyCoord(extended_sources, unit=(un.deg, un.deg))\n",
    "    \n",
    "    lcs_cut = []\n",
    "    for s, source in enumerate(lcfs):\n",
    "        lc = pd.read_csv(source)\n",
    "\n",
    "        lc_coord = SkyCoord(np.array([[np.nanmean(lc[ra_col]),\n",
    "                                       np.nanmean(lc[dec_col])]]),\n",
    "                            unit=(un.degree, un.degree))\n",
    "\n",
    "        seps = lc_coord.separation(es_sc)\n",
    "        min_sep = np.nanmin(seps.deg)\n",
    "\n",
    "        if min_sep > 3./60./60.:\n",
    "            lcs_cut.append(source)\n",
    "\n",
    "    return lcs_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_sn(lcfs, min_sn,\n",
    "              flux_col='f_int',\n",
    "              flux_err_col='f_int_err'):\n",
    "    '''\n",
    "    Remove light curves with S/N less than min_sn\n",
    "    \n",
    "    This function removes any sources that have\n",
    "    a signal to noise less than min_sn.\n",
    "    The fluxes of every source are divided by the\n",
    "    uncertainties to give the approximate\n",
    "    signal to noise. If there is no detection greater\n",
    "    than zero, or if there is no epoch where the source\n",
    "    has a signal to noise greater than (default) 2, then\n",
    "    the source is excluded from the analysis.\n",
    "    \n",
    "    Args:\n",
    "    lcfs (list): list of file names (including the\n",
    "                 path) of light curve files\n",
    "    min_sn (float): the minimum allowed signal\n",
    "                    to noise\n",
    "\n",
    "    Kwargs:\n",
    "    flux_col (str): The name of the flux column\n",
    "                    Default ='f_int'\n",
    "    flux_err_col (str): The name of the flux error column\n",
    "                        Default ='f_int_err'\n",
    "    Returns:\n",
    "    A list of filenames of sources that have\n",
    "    a signal to noise over min_sn\n",
    "    '''    \n",
    "    lcs_cut = []\n",
    "    for s, source in enumerate(lcfs):\n",
    "        lc = pd.read_csv(source)\n",
    "\n",
    "        # Get an approximation of the\n",
    "        # signal to noise for each epoch\n",
    "        signal_to_noise = lc[flux_col] / lc[flux_err_col]\n",
    "        \n",
    "        min_val = lc[flux_col] - lc[flux_err_col]\n",
    "\n",
    "        # If the source has at least one detection\n",
    "        # greater than the sinal to noise limit,\n",
    "        # include it in the analysis\n",
    "        if ((np.nanmax(signal_to_noise) > min_sn) and\n",
    "            (np.nanmax(min_val) > 0.)):\n",
    "            lcs_cut.append(source)\n",
    "    return lcs_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dpc(lcfs,\n",
    "               distance_from_phase_centre,\n",
    "               pc_col='dist_to_pc_DEG'):\n",
    "    '''\n",
    "    Filters sources that are\n",
    "    too distance from the phase centre\n",
    "    \n",
    "    This takes the file names of light curve files\n",
    "    and outputs a list of sources that are within\n",
    "    distance_from_phase_centre from the phase\n",
    "    centre\n",
    "    \n",
    "    Args:\n",
    "    lcfs (list): list of file names (including the\n",
    "                 path) of light curve files\n",
    "    distance_from_phase_centre (float): the maximum allowed\n",
    "                                        distance from the\n",
    "                                        phase centre that\n",
    "                                        a source can be\n",
    "                                        to be included\n",
    "                                        (in degrees)\n",
    "    kwargs:\n",
    "    pc_col (str): The name of the distance from phase\n",
    "                  centre column\n",
    "                  Default = 'dist_to_pc_DEG'\n",
    "\n",
    "    Returns:\n",
    "    A list of filenames of sources that are less than the\n",
    "    maximum distance from the phase centre\n",
    "    '''\n",
    "    lc_cut = []\n",
    "    for l, lcf in enumerate(lcfs):\n",
    "        # Read the light curve file\n",
    "        lc = pd.read_csv(lcf)\n",
    "        # Make sure the information\n",
    "        # is in chronological order\n",
    "        lc = lc.sort_values(by=[mjd_col])\n",
    "\n",
    "        pc_sep = np.nanmean(lc[pc_col])\n",
    "        if pc_sep <= distance_from_phase_centre:\n",
    "            lc_cut.append(lcf)\n",
    "\n",
    "    return lc_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_coords(lcfs, coords,\n",
    "                  ra_col='ra',\n",
    "                  dec_col='decl',\n",
    "                  sep=3./60./60.):\n",
    "    '''\n",
    "    Filters sources from a list of coordinates\n",
    "    \n",
    "    This takes the file names of light curve files\n",
    "    and outputs a list of sources that *don't*\n",
    "    match the coordinates of coords.\n",
    "    \n",
    "    Args:\n",
    "    lcfs (list): list of file names (including the\n",
    "                 path) of light curve files\n",
    "    coords (SkyCoord array): array of astropy SkyCoords\n",
    "                             of sources you want to\n",
    "                             filter out\n",
    "    kwargs:\n",
    "    ra_col (str): The name of the right ascension column\n",
    "                  Default = 'ra'\n",
    "    dec_col (str): The name of the declination column\n",
    "                   Default = 'decl'\n",
    "    sep (float): if two sources are less than sep (in degrees)\n",
    "                 apart they are considered to match\n",
    "\n",
    "    Returns:\n",
    "    A list of filenames of sources that *don't*\n",
    "    match the coordinates in coords\n",
    "    '''\n",
    "    lc_cut = []\n",
    "    for l, lcf in enumerate(lcfs):\n",
    "        # Read the light curve file\n",
    "        lc = pd.read_csv(lcf)\n",
    "        # Turn the source coordinates into an\n",
    "        # astropy SkyCoord object\n",
    "        lc_coord = SkyCoord(np.array([[np.nanmean(lc[ra_col]),\n",
    "                                       np.nanmean(lc[dec_col])]]),\n",
    "                            unit=(un.degree, un.degree))\n",
    "        # Check if the source is one of the\n",
    "        # known variables and move on if\n",
    "        # it is one\n",
    "        seps = lc_coord.separation(coords)\n",
    "        if np.nanmin(seps.deg)<sep:\n",
    "            print('Source {} is a known variable'.format(l))\n",
    "        else:\n",
    "            lc_cut.append(lcf)\n",
    "\n",
    "    return lc_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pearsons_r(flux0, flux1, remove_runcat=True):\n",
    "    '''\n",
    "    Get Pearson's r correlation coefficient for two arrays.\n",
    "    \n",
    "    This uses the scipy.stats.pearsonr function to\n",
    "    get the Pearson's r correlation coefficient between\n",
    "    two arrays, in this case lightcurves. It makes\n",
    "    sure that nan values don't mess up the correlation,\n",
    "    but removing nan values from both arrays. I.e. if\n",
    "    arrey0 has a nan at index 2, the value at index 2 is\n",
    "    removed from both arrays. Here we assume both arrays\n",
    "    are one-dimensional numpy arrays of the same length.\n",
    "    \n",
    "    Args:\n",
    "    flux0 (array): the light curve of the first source\n",
    "    flux1 (array): the light curve of the second source\n",
    "    \n",
    "    Returns:\n",
    "    Tuple: (pearson's r correlation coefficient,\n",
    "            two-tailed p-value)\n",
    "    If there is an issue with the arrays\n",
    "    then (np.nan, np.nan) will be\n",
    "    returned instead.\n",
    "    '''\n",
    "    # Remove the runcat from the start of each\n",
    "    # flux\n",
    "    if remove_runcat:\n",
    "        flux0 = flux0[1:]\n",
    "        flux1 = flux1[1:]\n",
    "    \n",
    "    # Check for infs and nans\n",
    "    flux0_nans = np.where(np.isnan(flux0))[0]\n",
    "    flux0_infs = np.where(np.isinf(flux0))[0]\n",
    "    flux1_nans = np.where(np.isnan(flux1))[0]\n",
    "    flux1_infs = np.where(np.isinf(flux1))[0]\n",
    "    \n",
    "    # Remove the infs and nans from both arrays\n",
    "    nans = np.unique(np.concatenate((flux0_nans, flux1_nans)))\n",
    "    infs = np.unique(np.concatenate((flux0_infs, flux1_infs)))\n",
    "    remove = np.concatenate((nans, infs))\n",
    "    try:\n",
    "        flux0_nn = np.delete(flux0, remove)\n",
    "        bob = True\n",
    "        try:\n",
    "            flux1_nn = np.delete(flux1, remove)\n",
    "            bob = True\n",
    "        except IndexError:\n",
    "            print('Flux wrong length: {}'.format(len(flux1)))\n",
    "            bob = False\n",
    "    except IndexError:\n",
    "        print('Flux wrong length: {}'.format(len(flux0)))\n",
    "        bob = False\n",
    "\n",
    "    # Get the Pearson's r and p-value\n",
    "    if bob:\n",
    "        try:\n",
    "            r = spstats.pearsonr(flux0_nn, flux1_nn)\n",
    "        except ValueError:\n",
    "            # If there's something wrong, you'll\n",
    "            # get back nans\n",
    "            print('Value Error')\n",
    "            print(s, len(flux0), len(flux1), len(notnans))\n",
    "            r = (np.nan, np.nan)\n",
    "    else:\n",
    "        r = (np.nan, np.nan)\n",
    "        \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lc_array(all_lcs,\n",
    "                  mjd_col='mjd',\n",
    "                  flux_col='f_int',\n",
    "                  flux_err_col='f_int_err',\n",
    "                  freq_col='freq_eff_int',\n",
    "                  pc_col='dist_to_pc_DEG'):\n",
    "    '''\n",
    "    Gather the info about each source that your need.\n",
    "    \n",
    "    To work out the correlations and test things you\n",
    "    need to have info about each source. It's much faster\n",
    "    and easier to gather all that stuff at the start,\n",
    "    rather than re-loading each source every time\n",
    "    you need it. It also means that your can do array-wise\n",
    "    calculations, which are again faster. So this function\n",
    "    grabs each source and puts its flux densities into\n",
    "    an array and also sticks the distance to phase centre,\n",
    "    median flux density, ra and dec of the source and puts\n",
    "    it into a pandas dataframe. The first column of the flux\n",
    "    density array is the runcat of each source. This is also\n",
    "    in the pandas array so that you can keep track of each\n",
    "    Source, but you have to remember to remove that column\n",
    "    when you do calculations!\n",
    "    \n",
    "    Args:\n",
    "    all_lcs (list): list of file names (including the path)\n",
    "                    of the light curve files for analysis\n",
    "\n",
    "    *kwargs:\n",
    "    min_sn (float): the minimum required signal to noise\n",
    "                    for a source to be considered detected\n",
    "                    Default: 2\n",
    "    mjd_col (str): the label for the MJD column in the\n",
    "                   lightcurve files\n",
    "    flux_col (str): the label for the flux density column\n",
    "                    in the lightcurve files\n",
    "    flux_err_col (str): the label for the flux density error\n",
    "                        column in the lightcurve files\n",
    "    freq_col (str): the label for the frequency column\n",
    "                    in the lightcurve files\n",
    "    pc_col (str): the label for the distance to phase centre\n",
    "                  column in the lightcurve files\n",
    "\n",
    "    Returns:\n",
    "    (flux, info)\n",
    "    flux: an array where each row is the flux density\n",
    "          over time of a source. The first column\n",
    "          contains the runcat of the source\n",
    "    info: a pandas dataframe with the runcat, distance\n",
    "          to phase centre, median flux density, ra,\n",
    "          and dec of each source\n",
    "    '''\n",
    "    for s, lcf in enumerate(all_lcs):\n",
    "        source_lc = pd.read_csv(lcf)\n",
    "\n",
    "        ra = np.nanmean(source_lc['ra'])\n",
    "        dec = np.nanmean(source_lc['decl'])\n",
    "\n",
    "        flux0 = np.array(source_lc[flux_col])\n",
    "        flux0_errs = np.array(source_lc[flux_err_col])\n",
    "\n",
    "        rcat = np.array(source_lc['runcat'])[0]\n",
    "        pc_dist = np.nanmean(source_lc[pc_col])\n",
    "\n",
    "        try:\n",
    "            flux0 = np.concatenate((np.array([rcat]),\n",
    "                                    flux0))\n",
    "            flux = np.concatenate((flux,\n",
    "                                   np.expand_dims(flux0, axis=0)))\n",
    "            source_info = np.concatenate((source_info,\n",
    "                                          np.array([[rcat,\n",
    "                                                     pc_dist,\n",
    "                                                     np.nanmedian(flux0),\n",
    "                                                     ra, dec]])))\n",
    "        except (NameError, UnboundLocalError):\n",
    "            flux = np.array(source_lc[flux_col])\n",
    "            flux = np.concatenate((np.array([rcat]),\n",
    "                                   flux))\n",
    "            flux = np.expand_dims(flux, axis=0)\n",
    "            source_info = np.array([[rcat,\n",
    "                                     pc_dist,\n",
    "                                     np.nanmedian(flux0),\n",
    "                                     ra, dec]])\n",
    "\n",
    "    return flux, pd.DataFrame(data=source_info, columns=['runcat',\n",
    "                                                         pc_col,\n",
    "                                                         'median {}'.format(flux_col),\n",
    "                                                         'ra', 'dec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corrs(source, other_sources):\n",
    "    '''\n",
    "    Calculate the Pearson's correlation coefficient\n",
    "    \n",
    "    This is using the formula from \n",
    "    https://docs.scipy.org/doc/scipy/reference/\n",
    "    generated/scipy.stats.pearsonr.html\n",
    "    \n",
    "    I chose to write my own function as the scipy\n",
    "    function doesn't have array-wise functionality,\n",
    "    which makes it really slow for calculating\n",
    "    a large amount of coefficients.\n",
    "    \n",
    "    Therefore, this function takes one light curve\n",
    "    and calculates the correlation coefficient\n",
    "    of that source with an array of light curves.\n",
    "    \n",
    "    Args:\n",
    "    source (array): the light curve of a single source\n",
    "    other_sources (array): an array of the light curves\n",
    "                           you want to compare to. Each row\n",
    "                           is a different light curve\n",
    "    \n",
    "    Returns:\n",
    "    An array containing the correlation coefficients\n",
    "    '''\n",
    "    x_mean = np.nanmean(source)\n",
    "    \n",
    "    y_mean = np.nanmean(other_sources, axis=1)\n",
    "    y_mean = np.transpose(np.tile(y_mean, (len(source), 1)))\n",
    "    \n",
    "    y_sub = other_sources - y_mean\n",
    "    y_sub_squared = y_sub ** 2.\n",
    "    \n",
    "    x_sub = np.tile((source - x_mean),\n",
    "                    (len(y_sub), 1))\n",
    "    x_sub_squared = np.tile(((source - x_mean) ** 2.),\n",
    "                            (len(y_sub), 1))\n",
    "    \n",
    "    top = np.nansum(x_sub*y_sub, axis=1)\n",
    "    bottom = np.sqrt(np.nansum(x_sub_squared, axis=1) *\n",
    "                     np.nansum(y_sub_squared, axis=1))\n",
    "    \n",
    "    return top/bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_all(fluxes, info,\n",
    "                mjd_col='mjd',\n",
    "                flux_col='f_int',\n",
    "                flux_err_col='f_int_err',\n",
    "                freq_col='freq_eff_int',\n",
    "                pc_col='dist_to_pc_DEG'):\n",
    "    '''\n",
    "    Compare every light curve to every other light curve\n",
    "    using the Pearson's r correlation coefficient\n",
    "    \n",
    "    Args:\n",
    "    fluxes (array): array of fluxes from make_lc_array\n",
    "    info (dataframe): pandas dataframe of information\n",
    "                      from make_lc_array\n",
    "    max_dpc (float): the maximum allowed distance from\n",
    "                     phase centre in degrees\n",
    "\n",
    "    *kwargs:\n",
    "    mjd_col (str): the label for the MJD column in the\n",
    "                   lightcurve files\n",
    "    flux_col (str): the label for the flux density column\n",
    "                    in the lightcurve files\n",
    "    flux_err_col (str): the label for the flux density error\n",
    "                        column in the lightcurve files\n",
    "    freq_col (str): the label for the frequency column\n",
    "                    in the lightcurve files\n",
    "    pc_col (str): the label for the distance to phase centre\n",
    "                  column in the lightcurve files\n",
    "\n",
    "    returns:\n",
    "    a dataframe with the results of the matches\n",
    "    '''\n",
    "    # Sort both the flux array and the info\n",
    "    # dataframe by runcat\n",
    "    flux_order = np.argsort(fluxes[:, 0])\n",
    "    fluxes_included = fluxes[flux_order]\n",
    "    inner_info = info.sort_values(by='runcat')\n",
    "    \n",
    "    results = np.ones((1, 11))\n",
    "    for s1, source1 in enumerate(np.array(inner_info['runcat'])):\n",
    "        # Set up the source you're going to compare\n",
    "        # all the other sources to\n",
    "        s1_loc = np.where(fluxes_included[:, 0] == source1)[0]\n",
    "        s1_flux = fluxes_included[s1_loc][0]\n",
    "        s1_flux = s1_flux[1:]\n",
    "        s1_info = inner_info[inner_info['runcat'] == source1]\n",
    "\n",
    "        # Delete the source you're comparing to\n",
    "        # (otherwise you'll get correlations of 1)\n",
    "        fluxes_included = np.delete(fluxes_included, s1_loc, axis=0)\n",
    "        runcats_included = np.copy(fluxes_included)[:, 0]\n",
    "\n",
    "        fluxes_for_corr = np.copy(fluxes_included)\n",
    "        fluxes_for_corr = fluxes_for_corr[:, 1:]\n",
    "        info_included = inner_info[inner_info['runcat'].isin(runcats_included)]\n",
    "        \n",
    "        # Correlate the source to all of the other sources\n",
    "        corrs = get_corrs(s1_flux, fluxes_for_corr)\n",
    "        \n",
    "        # Add the results to the results array\n",
    "        corr_results = np.ones((len(corrs), 11))\n",
    "        \n",
    "        corr_results[:, 0] = np.repeat(np.array(s1_info['runcat'])[0],\n",
    "                                       len(corr_results), axis=0)\n",
    "        corr_results[:, 1] = np.repeat(np.array(s1_info[pc_col])[0],\n",
    "                                       len(corr_results), axis=0)\n",
    "        corr_results[:, 2] = np.repeat(np.array(s1_info['ra'])[0],\n",
    "                                       len(corr_results), axis=0)\n",
    "        corr_results[:, 3] = np.repeat(np.array(s1_info['dec'])[0],\n",
    "                                       len(corr_results), axis=0)\n",
    "        corr_results[:, 4] = np.repeat(np.array(s1_info['median {}'.format(flux_col)])[0],\n",
    "                                       len(corr_results), axis=0)\n",
    "        \n",
    "        corr_results[:, 5] = np.array(info_included['runcat'])\n",
    "        corr_results[:, 6] = np.array(info_included[pc_col])\n",
    "        corr_results[:, 7] = np.array(info_included['ra'])\n",
    "        corr_results[:, 8] = np.array(info_included['dec'])\n",
    "        corr_results[:, 9] = np.array(info_included['median {}'.format(flux_col)])\n",
    "        \n",
    "        corr_results[:, 10] = corrs\n",
    "        \n",
    "        results = np.concatenate((results, corr_results))\n",
    "    results = results[1:]\n",
    "        \n",
    "    results_frame = pd.DataFrame(data=results,\n",
    "                                 columns=['s1_runcat',\n",
    "                                          's1_{}'.format(pc_col),\n",
    "                                          's1_ra', 's1_dec',\n",
    "                                          's1_median_{}'.format(flux_col),\n",
    "                                          's2_runcat',\n",
    "                                          's2_{}'.format(pc_col),\n",
    "                                          's2_ra', 's2_dec',\n",
    "                                          's2_median_{}'.format(flux_col),\n",
    "                                          'correlation coefficient'])\n",
    "    results_frame = results_frame[results_frame['s1_runcat'] !=\n",
    "                                  results_frame['s2_runcat']]\n",
    "    \n",
    "    return results_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The TraP database name\n",
    "db = 'Laura_NWLayers_ReProc'\n",
    "\n",
    "# The path to the light curves you want\n",
    "# to use\n",
    "lightcurve_path = ('/raid/driessen/FlareStars/'\n",
    "                   'GX339/Source_Light_Curves/'\n",
    "                   'Average_Scaled/')\n",
    "# The path to where files will be saved\n",
    "file_path = ('/raid/driessen/FlareStars/'\n",
    "             'GX339/Correlation_Investigation/')\n",
    "\n",
    "# The dataset ID values (from TraP) and\n",
    "# the corresponding central frequencies (MHz)\n",
    "DSs = [49, 52, 53, 50, 54, 55, 51, 33]\n",
    "freqs = [1658, 1551, 1444, 1337, 1123, 1016, 909, 'MFS']\n",
    "\n",
    "# The minimum signal to noise a source\n",
    "# needs to have (in at least one epoch)\n",
    "# to be included in the analysis\n",
    "minimum_sn = 2.\n",
    "\n",
    "# The coordinates of the extended sources in\n",
    "# the field\n",
    "es_coords = np.load(('/raid/driessen/FlareStars/'\n",
    "                     'GX339/2021.01.18_GX339_KnownExtendedSources.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "Working on 1658 MHz, ds 49\n",
      "******************\n",
      "Running ES\n",
      "Working on f_int f_int_err\n",
      "Shape of flux array:  (1676, 103)\n",
      "*** f_int Done ***\n",
      "Working on f_int_median_scaled f_int_median_scaled_err\n",
      "Shape of flux array:  (1676, 103)\n",
      "*** f_int_median_scaled Done ***\n",
      "^^^ ES Done ^^^\n",
      "******************\n",
      "Running PS\n",
      "Working on f_int f_int_err\n",
      "Shape of flux array:  (1281, 103)\n",
      "*** f_int Done ***\n",
      "Working on f_int_median_scaled f_int_median_scaled_err\n",
      "Shape of flux array:  (1281, 103)\n",
      "*** f_int_median_scaled Done ***\n",
      "^^^ PS Done ^^^\n",
      "--- 1658 Done ---\n",
      "\n",
      "----------------------------------\n",
      "\n",
      "----------------------------------\n",
      "Working on 1551 MHz, ds 52\n",
      "******************\n",
      "Running ES\n",
      "Working on f_int f_int_err\n",
      "Shape of flux array:  (2062, 103)\n",
      "*** f_int Done ***\n",
      "Working on f_int_median_scaled f_int_median_scaled_err\n",
      "Shape of flux array:  (2062, 103)\n",
      "*** f_int_median_scaled Done ***\n",
      "^^^ ES Done ^^^\n",
      "******************\n",
      "Running PS\n",
      "Working on f_int f_int_err\n",
      "Shape of flux array:  (1630, 103)\n",
      "*** f_int Done ***\n",
      "Working on f_int_median_scaled f_int_median_scaled_err\n",
      "Shape of flux array:  (1630, 103)\n",
      "*** f_int_median_scaled Done ***\n",
      "^^^ PS Done ^^^\n",
      "--- 1551 Done ---\n",
      "\n",
      "----------------------------------\n",
      "\n",
      "----------------------------------\n",
      "Working on 1444 MHz, ds 53\n",
      "******************\n",
      "Running ES\n",
      "Working on f_int f_int_err\n",
      "Shape of flux array:  (2633, 103)\n",
      "*** f_int Done ***\n",
      "Working on f_int_median_scaled f_int_median_scaled_err\n",
      "Shape of flux array:  (2633, 103)\n",
      "*** f_int_median_scaled Done ***\n",
      "^^^ ES Done ^^^\n",
      "******************\n",
      "Running PS\n",
      "Working on f_int f_int_err\n",
      "Shape of flux array:  (2064, 103)\n",
      "*** f_int Done ***\n",
      "Working on f_int_median_scaled f_int_median_scaled_err\n",
      "Shape of flux array:  (2064, 103)\n",
      "*** f_int_median_scaled Done ***\n",
      "^^^ PS Done ^^^\n",
      "--- 1444 Done ---\n",
      "\n",
      "----------------------------------\n",
      "\n",
      "----------------------------------\n",
      "Working on 1337 MHz, ds 50\n",
      "******************\n",
      "Running ES\n",
      "Working on f_int f_int_err\n",
      "Shape of flux array:  (2725, 103)\n",
      "*** f_int Done ***\n",
      "Working on f_int_median_scaled f_int_median_scaled_err\n",
      "Shape of flux array:  (2725, 103)\n",
      "*** f_int_median_scaled Done ***\n",
      "^^^ ES Done ^^^\n",
      "******************\n",
      "Running PS\n",
      "Working on f_int f_int_err\n",
      "Shape of flux array:  (2161, 103)\n",
      "*** f_int Done ***\n",
      "Working on f_int_median_scaled f_int_median_scaled_err\n",
      "Shape of flux array:  (2161, 103)\n",
      "*** f_int_median_scaled Done ***\n",
      "^^^ PS Done ^^^\n",
      "--- 1337 Done ---\n",
      "\n",
      "----------------------------------\n",
      "\n",
      "----------------------------------\n",
      "Working on 1123 MHz, ds 54\n",
      "******************\n",
      "Running ES\n",
      "Working on f_int f_int_err\n",
      "Shape of flux array:  (2836, 103)\n",
      "*** f_int Done ***\n",
      "Working on f_int_median_scaled f_int_median_scaled_err\n",
      "Shape of flux array:  (2836, 103)\n",
      "*** f_int_median_scaled Done ***\n",
      "^^^ ES Done ^^^\n",
      "******************\n",
      "Running PS\n",
      "Working on f_int f_int_err\n",
      "Shape of flux array:  (2213, 103)\n",
      "*** f_int Done ***\n",
      "Working on f_int_median_scaled f_int_median_scaled_err\n",
      "Shape of flux array:  (2213, 103)\n",
      "*** f_int_median_scaled Done ***\n",
      "^^^ PS Done ^^^\n",
      "--- 1123 Done ---\n",
      "\n",
      "----------------------------------\n",
      "\n",
      "----------------------------------\n",
      "Working on 1016 MHz, ds 55\n",
      "******************\n",
      "Running ES\n",
      "Working on f_int f_int_err\n",
      "Shape of flux array:  (3266, 103)\n",
      "*** f_int Done ***\n",
      "Working on f_int_median_scaled f_int_median_scaled_err\n",
      "Shape of flux array:  (3266, 103)\n",
      "*** f_int_median_scaled Done ***\n",
      "^^^ ES Done ^^^\n",
      "******************\n",
      "Running PS\n",
      "Working on f_int f_int_err\n",
      "Shape of flux array:  (2565, 103)\n",
      "*** f_int Done ***\n",
      "Working on f_int_median_scaled f_int_median_scaled_err\n",
      "Shape of flux array:  (2565, 103)\n",
      "*** f_int_median_scaled Done ***\n",
      "^^^ PS Done ^^^\n",
      "--- 1016 Done ---\n",
      "\n",
      "----------------------------------\n",
      "\n",
      "----------------------------------\n",
      "Working on 909 MHz, ds 51\n",
      "******************\n",
      "Running ES\n",
      "Working on f_int f_int_err\n",
      "Shape of flux array:  (2608, 103)\n",
      "*** f_int Done ***\n",
      "Working on f_int_median_scaled f_int_median_scaled_err\n",
      "Shape of flux array:  (2608, 103)\n",
      "*** f_int_median_scaled Done ***\n",
      "^^^ ES Done ^^^\n",
      "******************\n",
      "Running PS\n",
      "Working on f_int f_int_err\n",
      "Shape of flux array:  (1996, 103)\n",
      "*** f_int Done ***\n",
      "Working on f_int_median_scaled f_int_median_scaled_err\n",
      "Shape of flux array:  (1996, 103)\n",
      "*** f_int_median_scaled Done ***\n",
      "^^^ PS Done ^^^\n",
      "--- 909 Done ---\n",
      "\n",
      "----------------------------------\n",
      "\n",
      "----------------------------------\n",
      "Working on MFS MHz, ds 33\n",
      "******************\n",
      "Running ES\n",
      "Working on f_int f_int_err\n",
      "Shape of flux array:  (4300, 102)\n",
      "*** f_int Done ***\n",
      "Working on f_int_median_scaled f_int_median_scaled_err\n",
      "Shape of flux array:  (4300, 102)\n",
      "*** f_int_median_scaled Done ***\n",
      "^^^ ES Done ^^^\n",
      "******************\n",
      "Running PS\n",
      "Working on f_int f_int_err\n",
      "Shape of flux array:  (3608, 102)\n",
      "*** f_int Done ***\n",
      "Working on f_int_median_scaled f_int_median_scaled_err\n",
      "Shape of flux array:  (3608, 102)\n",
      "*** f_int_median_scaled Done ***\n",
      "^^^ PS Done ^^^\n",
      "--- MFS Done ---\n",
      "\n",
      "----------------------------------\n",
      "\n",
      "\n",
      "******************************\n",
      "Done\n",
      "\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "# Make or load the flux array and info dataframe\n",
    "for d, ds in enumerate(DSs):\n",
    "    print('----------------------------------')\n",
    "    freq = freqs[d]\n",
    "    print('Working on {0} MHz, ds {1}'.format(freq, ds))\n",
    "\n",
    "    for e, ext in enumerate(['ES', 'PS']):\n",
    "        print('******************')\n",
    "        print('Running {}'.format(ext))\n",
    "\n",
    "        all_lcs = sorted(glob.glob(('{0}rcat*_'\n",
    "                                    'ra*_dec*_'\n",
    "                                    'db{1}_ds{2}_'\n",
    "                                    '{3}.csv').format(lightcurve_path,\n",
    "                                                      db,\n",
    "                                                      ds,\n",
    "                                                      ext)))\n",
    "        sn_cut_lcs = filter_sn(all_lcs, minimum_sn)\n",
    "\n",
    "        for f, fc in enumerate(['f_int', 'f_int_median_scaled']):\n",
    "            fc_err = '{}_err'.format(fc)\n",
    "            print('Working on {0} {1}'.format(fc, fc_err))\n",
    "\n",
    "            flux_filename = ('{0}AllFluxes_'\n",
    "                             '{1}MHz_'\n",
    "                             '{2}_'\n",
    "                             'db{3}_'\n",
    "                             'ds{4}_'\n",
    "                             '{5}_'\n",
    "                             'minSN{6}').format(file_path,\n",
    "                                                freq, fc,\n",
    "                                                db, ds, ext,\n",
    "                                                minimum_sn)\n",
    "            info_filename = ('{0}AllInfo_'\n",
    "                             '{1}MHz_'\n",
    "                             '{2}_'\n",
    "                             'db{3}_'\n",
    "                             'ds{4}_'\n",
    "                             '{5}'\n",
    "                             'minSN{6}.csv').format(file_path,\n",
    "                                                    freq, fc,\n",
    "                                                    db, ds, ext,\n",
    "                                                    minimum_sn)\n",
    "\n",
    "            flux, info = make_lc_array(sn_cut_lcs,\n",
    "                                       flux_col=fc,\n",
    "                                       flux_err_col=fc_err)\n",
    "            print('Shape of flux array: ', np.shape(flux))\n",
    "            np.save(flux_filename, flux)\n",
    "            info.to_csv(info_filename)\n",
    "\n",
    "            print('*** {} Done ***'.format(fc))\n",
    "        print('^^^ {} Done ^^^'.format(ext))\n",
    "    print('--- {} Done ---'.format(freq))\n",
    "    print('\\n----------------------------------\\n')\n",
    "print('\\n******************************')\n",
    "print('Done')\n",
    "print('\\n******************************')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "Working on 1658 MHz, ds 49\n",
      "******************\n",
      "Running ES\n",
      "Working on f_int f_int_err\n",
      "Min corr: 0.977190\n",
      "Max corr: -0.590454\n",
      "*** f_int Done ***\n",
      "Working on f_int_median_scaled f_int_median_scaled_err\n",
      "Min corr: 0.953921\n",
      "Max corr: -0.808516\n",
      "*** f_int_median_scaled Done ***\n",
      "^^^ ES Done ^^^\n",
      "******************\n",
      "Running PS\n",
      "Working on f_int f_int_err\n",
      "Min corr: 0.747664\n",
      "Max corr: -0.590454\n",
      "*** f_int Done ***\n",
      "Working on f_int_median_scaled f_int_median_scaled_err\n",
      "Min corr: 0.770478\n",
      "Max corr: -0.648907\n",
      "*** f_int_median_scaled Done ***\n",
      "^^^ PS Done ^^^\n",
      "--- 1658 Done ---\n",
      "\n",
      "----------------------------------\n",
      "\n",
      "----------------------------------\n",
      "Working on 1551 MHz, ds 52\n",
      "******************\n",
      "Running ES\n",
      "Working on f_int f_int_err\n",
      "Min corr: 0.979168\n",
      "Max corr: -0.681503\n",
      "*** f_int Done ***\n",
      "Working on f_int_median_scaled f_int_median_scaled_err\n",
      "Min corr: 0.963980\n",
      "Max corr: -0.771834\n",
      "*** f_int_median_scaled Done ***\n",
      "^^^ ES Done ^^^\n",
      "******************\n",
      "Running PS\n",
      "Working on f_int f_int_err\n",
      "Min corr: 0.711930\n",
      "Max corr: -0.681503\n",
      "*** f_int Done ***\n",
      "Working on f_int_median_scaled f_int_median_scaled_err\n",
      "Min corr: 0.766747\n",
      "Max corr: -0.723019\n",
      "*** f_int_median_scaled Done ***\n",
      "^^^ PS Done ^^^\n",
      "--- 1551 Done ---\n",
      "\n",
      "----------------------------------\n",
      "\n",
      "----------------------------------\n",
      "Working on 1444 MHz, ds 53\n",
      "******************\n",
      "Running ES\n",
      "Working on f_int f_int_err\n",
      "Min corr: 0.992074\n",
      "Max corr: -0.823027\n",
      "*** f_int Done ***\n",
      "Working on f_int_median_scaled f_int_median_scaled_err\n",
      "Min corr: 0.989364\n",
      "Max corr: -0.914588\n",
      "*** f_int_median_scaled Done ***\n",
      "^^^ ES Done ^^^\n",
      "******************\n",
      "Running PS\n",
      "Working on f_int f_int_err\n",
      "Min corr: 0.911578\n",
      "Max corr: -0.649235\n",
      "*** f_int Done ***\n",
      "Working on f_int_median_scaled f_int_median_scaled_err\n",
      "Min corr: 0.851072\n",
      "Max corr: -0.802917\n",
      "*** f_int_median_scaled Done ***\n",
      "^^^ PS Done ^^^\n",
      "--- 1444 Done ---\n",
      "\n",
      "----------------------------------\n",
      "\n",
      "----------------------------------\n",
      "Working on 1337 MHz, ds 50\n",
      "******************\n",
      "Running ES\n",
      "Working on f_int f_int_err\n",
      "Min corr: 0.983584\n",
      "Max corr: -0.654908\n",
      "*** f_int Done ***\n",
      "Working on f_int_median_scaled f_int_median_scaled_err\n",
      "Min corr: 0.960282\n",
      "Max corr: -0.914981\n",
      "*** f_int_median_scaled Done ***\n",
      "^^^ ES Done ^^^\n",
      "******************\n",
      "Running PS\n",
      "Working on f_int f_int_err\n",
      "Min corr: 0.860257\n",
      "Max corr: -0.654908\n",
      "*** f_int Done ***\n",
      "Working on f_int_median_scaled f_int_median_scaled_err\n",
      "Min corr: 0.895392\n",
      "Max corr: -0.890503\n",
      "*** f_int_median_scaled Done ***\n",
      "^^^ PS Done ^^^\n",
      "--- 1337 Done ---\n",
      "\n",
      "----------------------------------\n",
      "\n",
      "----------------------------------\n",
      "Working on 1123 MHz, ds 54\n",
      "******************\n",
      "Running ES\n",
      "Working on f_int f_int_err\n",
      "Min corr: 0.975877\n",
      "Max corr: -0.778870\n",
      "*** f_int Done ***\n",
      "Working on f_int_median_scaled f_int_median_scaled_err\n",
      "Min corr: 0.961365\n",
      "Max corr: -0.895174\n",
      "*** f_int_median_scaled Done ***\n",
      "^^^ ES Done ^^^\n",
      "******************\n",
      "Running PS\n",
      "Working on f_int f_int_err\n",
      "Min corr: 0.938531\n",
      "Max corr: -0.778870\n",
      "*** f_int Done ***\n",
      "Working on f_int_median_scaled f_int_median_scaled_err\n",
      "Min corr: 0.892044\n",
      "Max corr: -0.839589\n",
      "*** f_int_median_scaled Done ***\n",
      "^^^ PS Done ^^^\n",
      "--- 1123 Done ---\n",
      "\n",
      "----------------------------------\n",
      "\n",
      "----------------------------------\n",
      "Working on 1016 MHz, ds 55\n",
      "******************\n",
      "Running ES\n",
      "Working on f_int f_int_err\n",
      "Min corr: 0.981239\n",
      "Max corr: -0.758433\n",
      "*** f_int Done ***\n",
      "Working on f_int_median_scaled f_int_median_scaled_err\n",
      "Min corr: 0.980765\n",
      "Max corr: -0.921034\n",
      "*** f_int_median_scaled Done ***\n",
      "^^^ ES Done ^^^\n",
      "******************\n",
      "Running PS\n",
      "Working on f_int f_int_err\n",
      "Min corr: 0.940725\n",
      "Max corr: -0.758433\n",
      "*** f_int Done ***\n",
      "Working on f_int_median_scaled f_int_median_scaled_err\n",
      "Min corr: 0.949733\n",
      "Max corr: -0.883625\n",
      "*** f_int_median_scaled Done ***\n",
      "^^^ PS Done ^^^\n",
      "--- 1016 Done ---\n",
      "\n",
      "----------------------------------\n",
      "\n",
      "----------------------------------\n",
      "Working on 909 MHz, ds 51\n",
      "******************\n",
      "Running ES\n",
      "Working on f_int f_int_err\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/driessen/.conda/envs/LaurasJupyter36/lib/python3.6/site-packages/ipykernel_launcher.py:44: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min corr: 0.987593\n",
      "Max corr: -0.650588\n",
      "*** f_int Done ***\n",
      "Working on f_int_median_scaled f_int_median_scaled_err\n",
      "Min corr: 0.986641\n",
      "Max corr: -0.879129\n",
      "*** f_int_median_scaled Done ***\n",
      "^^^ ES Done ^^^\n",
      "******************\n",
      "Running PS\n",
      "Working on f_int f_int_err\n",
      "Min corr: 0.895510\n",
      "Max corr: -0.587954\n",
      "*** f_int Done ***\n",
      "Working on f_int_median_scaled f_int_median_scaled_err\n",
      "Min corr: 0.928921\n",
      "Max corr: -0.741402\n",
      "*** f_int_median_scaled Done ***\n",
      "^^^ PS Done ^^^\n",
      "--- 909 Done ---\n",
      "\n",
      "----------------------------------\n",
      "\n",
      "----------------------------------\n",
      "Working on MFS MHz, ds 33\n",
      "******************\n",
      "Running ES\n",
      "Working on f_int f_int_err\n",
      "Min corr: 0.993124\n",
      "Max corr: -0.730133\n",
      "*** f_int Done ***\n",
      "Working on f_int_median_scaled f_int_median_scaled_err\n",
      "Min corr: 0.981400\n",
      "Max corr: -0.858616\n",
      "*** f_int_median_scaled Done ***\n",
      "^^^ ES Done ^^^\n",
      "******************\n",
      "Running PS\n",
      "Working on f_int f_int_err\n",
      "Min corr: 0.969814\n",
      "Max corr: -0.730133\n",
      "*** f_int Done ***\n",
      "Working on f_int_median_scaled f_int_median_scaled_err\n",
      "Min corr: 0.951160\n",
      "Max corr: -0.847111\n",
      "*** f_int_median_scaled Done ***\n",
      "^^^ PS Done ^^^\n",
      "--- MFS Done ---\n",
      "\n",
      "----------------------------------\n",
      "\n",
      "\n",
      "******************************\n",
      "\n",
      "Done\n",
      "\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "# Perform the correlations\n",
    "for d, ds in enumerate(DSs):\n",
    "    print('----------------------------------')\n",
    "    freq = freqs[d]\n",
    "    print('Working on {0} MHz, ds {1}'.format(freq, ds))\n",
    "\n",
    "    for e, ext in enumerate(['ES', 'PS']):\n",
    "        print('******************')\n",
    "        print('Running {}'.format(ext))\n",
    "\n",
    "        for f, fc in enumerate(['f_int', 'f_int_median_scaled']):\n",
    "            fc_err = '{}_err'.format(fc)\n",
    "            print('Working on {0} {1}'.format(fc, fc_err))\n",
    "\n",
    "            flux_filename = ('{0}AllFluxes_'\n",
    "                             '{1}MHz_'\n",
    "                             '{2}_'\n",
    "                             'db{3}_'\n",
    "                             'ds{4}_'\n",
    "                             '{5}_'\n",
    "                             'minSN{6}.npy').format(file_path,\n",
    "                                                    freq, fc,\n",
    "                                                    db, ds, ext,\n",
    "                                                    minimum_sn)\n",
    "            info_filename = ('{0}AllInfo_'\n",
    "                             '{1}MHz_'\n",
    "                             '{2}_'\n",
    "                             'db{3}_'\n",
    "                             'ds{4}_'\n",
    "                             '{5}'\n",
    "                             'minSN{6}.csv').format(file_path,\n",
    "                                                    freq, fc,\n",
    "                                                    db, ds, ext,\n",
    "                                                    minimum_sn)\n",
    "\n",
    "            correlation_filename = ('{0}AllCorrelations_{1}MHz_'\n",
    "                                    '{2}_'\n",
    "                                    'db{3}_'\n",
    "                                    'ds{4}_'\n",
    "                                    '{5}_'\n",
    "                                    'minSN{6}.csv').format(file_path,\n",
    "                                                           freq, fc, \n",
    "                                                           db, ds,\n",
    "                                                           ext,\n",
    "                                                           minimum_sn)\n",
    "\n",
    "            fluxes = np.load(flux_filename)\n",
    "            info = pd.read_csv(info_filename)\n",
    "\n",
    "            all_correlations = compare_all(fluxes, info,\n",
    "                                           flux_col=fc,\n",
    "                                           flux_err_col=fc_err)\n",
    "\n",
    "            print('Min corr: {:4f}'.format(np.max(all_correlations['correlation coefficient'])))\n",
    "            print('Max corr: {:4f}'.format(np.min(all_correlations['correlation coefficient'])))\n",
    "\n",
    "            all_correlations.to_csv(correlation_filename)\n",
    "\n",
    "            print('*** {} Done ***'.format(fc))\n",
    "        print('^^^ {} Done ^^^'.format(ext))\n",
    "    print('--- {} Done ---'.format(freq))\n",
    "    print('\\n----------------------------------\\n')\n",
    "print('\\n******************************\\n')\n",
    "print('Done')\n",
    "print('\\n******************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
