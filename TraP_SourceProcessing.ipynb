{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "from astropy import units as un\n",
    "from astropy.coordinates import SkyCoord\n",
    "import astropy.time\n",
    "from astropy.io import fits\n",
    "from astropy import nddata\n",
    "from astropy.wcs import WCS\n",
    "\n",
    "import tkp.config\n",
    "import sqlalchemy\n",
    "from sqlalchemy import *\n",
    "from sqlalchemy.orm import relationship\n",
    "import tkp.db\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "query_loglevel = logging.WARNING  # Set to INFO to see queries, otherwise WARNING\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('/scratch/ldriessen/TraP_tools')\n",
    "sys.path.append('/scratch/ldriessen/TraP_tools/databaseTools/')\n",
    "sys.path.append('/scratch/ldriessen/TraP_tools/tools/')\n",
    "sys.path.append('/scratch/ldriessen/TraP_tools/exampleScripts/')\n",
    "sys.path.append('/scratch/ldriessen/TraP_tools/plotting/')\n",
    "from dblogin import * # This file contains all the variables required to connect to the database\n",
    "from databaseTools import dbtools\n",
    "from tools import tools\n",
    "from plotting import plot_varib_params as pltvp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def madulation_parameter(data):\n",
    "    '''\n",
    "    Use the median and mad to make a modulation-type parameter.\n",
    "\n",
    "    Args:\n",
    "    data (array): the 1D array\n",
    "                  of data that you\n",
    "                  want to find the\n",
    "                  MAD parameter of\n",
    "    Returns:\n",
    "    (madvalue, madlocation)\n",
    "    The value of the MAD parameter\n",
    "    and the index of the epoch\n",
    "    of the MAD parameter\n",
    "    '''\n",
    "    me = np.nanmedian(data)\n",
    "    ma = np.nanmedian(np.absolute(data-np.nanmedian(data)))\n",
    "    \n",
    "    data_ = np.copy(data)\n",
    "    \n",
    "    shifted_data = np.abs(data_ - me)\n",
    "    divided_by_mad = shifted_data/ma\n",
    "    \n",
    "    try:\n",
    "        madvalue = np.nanmax(divided_by_mad)\n",
    "        madlocation = np.nanargmax(divided_by_mad)\n",
    "    except ValueError:\n",
    "        madvalue = np.nan\n",
    "        madlocation = np.nan\n",
    "    \n",
    "    return madvalue, madlocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi2_parameter(data, uncertainties):\n",
    "    '''\n",
    "    Calculate the chi2 variability parameter for a set of points.\n",
    "    \n",
    "    A description of the variability parameter\n",
    "    can be found here:\n",
    "    https://tkp.readthedocs.io/en/r3.0/devref/\n",
    "    database/schema.html#appendices\n",
    "    \n",
    "    Args:\n",
    "    data (array): a 1D row array of data points\n",
    "    uncertainties (array): the 1D array of uncertainties\n",
    "                           that corresponds to the data array\n",
    "    \n",
    "    Returns:\n",
    "    A float that is the value of the variability\n",
    "    parameter for the data array\n",
    "    '''\n",
    "    weights = 1./(uncertainties**2.)\n",
    "    \n",
    "    p1 = len(data[~np.isnan(data)])/(len(data[~np.isnan(data)])-1.)\n",
    "    p2 = np.nanmean(weights*(data**2.))\n",
    "    p3 = ((np.nanmean(weights*data))**2.)/(np.nanmean(weights))\n",
    "    \n",
    "    return p1 * (p2 - p3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modulation_parameter(data):\n",
    "    '''\n",
    "    Calculate the modulation parameter for a set of points.\n",
    "    \n",
    "    A description of the modulation parameter\n",
    "    can be found here:\n",
    "    https://tkp.readthedocs.io/en/r3.0/devref/\n",
    "    database/schema.html#appendices\n",
    "    \n",
    "    Args:\n",
    "    data (array): a 1D row array of data points\n",
    "    \n",
    "    Returns:\n",
    "    A float that is the value of the modulation\n",
    "    parameter for the data array\n",
    "    '''\n",
    "    p1 = 1./np.nanmean(data)\n",
    "    p2 = len(data)/(len(data)-1)\n",
    "    p3 = np.nanmean(data**2.) - (np.nanmean(data))**2.\n",
    "    \n",
    "    return p1 * np.sqrt(p2*p3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query(transients_query, dbname, dataset_id,\n",
    "              engine, host, port, user, pword):\n",
    "    '''\n",
    "    Function to get information from TraP directly.\n",
    "    \n",
    "    This function accesses a TraP database and gets\n",
    "    the information (in transients query).\n",
    "    \n",
    "    Args:\n",
    "    transients query: This is a weird format thing\n",
    "                      that contains the keywords for\n",
    "                      what you would like from TraP.\n",
    "                      For example:\n",
    "        transients_query = \"\"\"SELECT \n",
    "        ex.f_peak,\n",
    "        ex.f_peak_err,\n",
    "        ex.ra,\n",
    "        ex.ra_err,\n",
    "        ex.decl,\n",
    "        ex.decl_err\n",
    "        FROM extractedsource ex, image im \n",
    "        WHERE ex.id IN ( select xtrsrc FROM assocxtrsource WHERE runcat = {}) \n",
    "        AND ex.image = im.id\n",
    "        ORDER BY ex.det_sigma DESC;\n",
    "        \"\"\".format(runcat)\n",
    "    dbname (str): the name of the TraP database you'd like to access\n",
    "    dataset_id (int): the number of the data set in the database that you'd\n",
    "                      like to access\n",
    "    engine (str): a TraP thing, going to be 'postgresql'\n",
    "    host (str): the host url ('vlo.science.uva.nl')\n",
    "    port (int): the port to connect to (5432)\n",
    "    user (str): your TraP username\n",
    "    pword (str): your TraP password\n",
    "    \n",
    "    Returns:\n",
    "    A Pandas table containing the information you requested\n",
    "    '''\n",
    "    tkp.db.Database(\n",
    "        database=dbname, user=user, password=pword,\n",
    "        engine=engine, host=host, port=port\n",
    "    )\n",
    "\n",
    "    cursor = tkp.db.execute(transients_query, (dataset_id,))\n",
    "    transients = tkp.db.generic.get_db_rows_as_dicts(cursor)\n",
    "    return transients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lightcurves(phase_centre_ra,\n",
    "                    phase_centre_dec,\n",
    "                    min_dpts,\n",
    "                    engine,\n",
    "                    host,\n",
    "                    port,\n",
    "                    user,\n",
    "                    password,\n",
    "                    database,\n",
    "                    dataset_id,\n",
    "                    image_values,\n",
    "                    drop_mjd='None',\n",
    "                    flux_col='f_int'):\n",
    "    '''\n",
    "    Get the light curves from a TraP dataset and add some parameters.\n",
    "    \n",
    "    This script accesses the banana database and gets the light curves\n",
    "    from the database as pandas tables. It makes a list of all the light\n",
    "    curve tables from the dataset.\n",
    "    \n",
    "    Args:\n",
    "    phase_centre_ra (float): the Right Ascension of the phase centre of\n",
    "                             the images you've processed with TraP\n",
    "                             in degrees\n",
    "    phase_centre_dec (float): the Declination of the phase centre of the\n",
    "                              the images you've processed with TraP\n",
    "                              in degrees\n",
    "    mid_dpts (int): the minimum number of detections of the source\n",
    "                    by TraP\n",
    "    dbname (str): the name of the TraP database you'd like to access\n",
    "    dataset_id (int): the number of the data set in the database that you'd\n",
    "                      like to access\n",
    "    engine (str): a TraP thing, going to be 'postgresql'\n",
    "    host (str): the host url ('vlo.science.uva.nl')\n",
    "    port (int): the port to connect to (5432)\n",
    "    user (str): your TraP username\n",
    "    pword (str): your TraP password\n",
    "    \n",
    "    Kwargs:\n",
    "    drop_mjd (int/float): the MJD of the epoch you want\n",
    "                          to exclude completely. This is\n",
    "                          important if you have artificially\n",
    "                          included a deep image at the start\n",
    "                          of the TraP run.\n",
    "                          Default: 'None' (no epoch is removed)\n",
    "    flux_col (str): the nameof the flux column you want to\n",
    "                    use. This is important because TraP records\n",
    "                    the integrated ('f_int') and peak ('f_peak')\n",
    "                    flux densities for all the sources.\n",
    "                    Default: 'f_int'\n",
    "    \n",
    "    Returns:\n",
    "    A list of pandas tables, where each pandas table contains\n",
    "    information about a source from TraP\n",
    "    '''\n",
    "    # The coordinates of the phase centre of the image    \n",
    "    phase_centre = SkyCoord(phase_centre_ra*un.degree,\n",
    "                            phase_centre_dec*un.degree,\n",
    "                            frame='icrs')\n",
    "\n",
    "    # Get the basic values for each source in the dataset\n",
    "    # by accessing the TraP\n",
    "    session = dbtools.access(engine,host,port,user,password,database)\n",
    "    VarParams = dbtools.GetVarParams(session, dataset_id)\n",
    "    plotdata = [[VarParams[i].Runningcatalog.id,\n",
    "                 VarParams[i].Varmetric.eta_int,\n",
    "                 VarParams[i].Varmetric.v_int,\n",
    "                 VarParams[i].Varmetric.lightcurve_max,\n",
    "                 VarParams[i].Varmetric.lightcurve_median,\n",
    "                 (VarParams[i].Varmetric.band.freq_central/1e6),\n",
    "                 VarParams[i].Runningcatalog.datapoints,\n",
    "                 VarParams[i].Varmetric.newsource,\n",
    "                 VarParams[i].Runningcatalog.wm_ra,\n",
    "                 VarParams[i].Runningcatalog.wm_decl]\n",
    "                for i in range(len(VarParams))]\n",
    "\n",
    "    plotdata = pd.DataFrame(data=plotdata,columns=['runcat',\n",
    "                                                   'eta',\n",
    "                                                   'V',\n",
    "                                                   'maxFlx',\n",
    "                                                   'avgFlx',\n",
    "                                                   'freq',\n",
    "                                                   'dpts',\n",
    "                                                   'newSrc',\n",
    "                                                   'ra',\n",
    "                                                   'dec'])\n",
    "    plotdata = plotdata.fillna('N')\n",
    "    plotdata = plotdata.loc[(plotdata['dpts']>=min_dpts)]\n",
    "    num_measurements = np.argmax(np.array(plotdata['dpts']))\n",
    "\n",
    "    runcats = np.array(plotdata['runcat'])\n",
    "    Vs = np.array(plotdata['V'])\n",
    "    etas = np.array(plotdata['eta'])\n",
    "\n",
    "    del session\n",
    "    \n",
    "    # Connect a new session to the TraP database\n",
    "    db = tkp.db.Database(engine=engine, host=host, port=port,\n",
    "                         user=user, password=password,\n",
    "                         database=database)\n",
    "    db.connect()\n",
    "    session = db.Session()\n",
    "\n",
    "    # Set up to make a list of source panda tables\n",
    "    source_lightcurves = []\n",
    "    # Get the light curve and other information\n",
    "    # for each running catalogue (source)\n",
    "    for r, runcat in enumerate(runcats):\n",
    "        # These are the values for the source that\n",
    "        # will be in its pandas table\n",
    "        transients_query = \"\"\"SELECT\n",
    "        ex.f_int,\n",
    "        ex.f_int_err,\n",
    "        ex.f_peak,\n",
    "        ex.f_peak_err,\n",
    "        ex.ra,\n",
    "        ex.ra_err,\n",
    "        ex.decl,\n",
    "        ex.decl_err,\n",
    "        im.band,\n",
    "        im.freq_eff,\n",
    "        im.taustart_ts\n",
    "        FROM extractedsource ex, image im \n",
    "        WHERE ex.id IN ( select xtrsrc FROM assocxtrsource WHERE runcat = {}) \n",
    "        AND ex.image = im.id\n",
    "        ORDER BY ex.det_sigma DESC;\n",
    "        \"\"\".format(runcat)\n",
    "        # Get the pandas table\n",
    "        source_data = pd.DataFrame(run_query(transients_query,\n",
    "                                             database,\n",
    "                                             dataset_id, \n",
    "                                             engine, host,\n",
    "                                             port, user,\n",
    "                                             password))\n",
    "        # The centre frequency can vary by a couple\n",
    "        # of kHz depending on whether the data were taken\n",
    "        # in 1k, 4k, or 32k mode. Using the integer\n",
    "        # flux density helps keep everything together\n",
    "        # even if the centre frequency isn't exactly the same\n",
    "        source_data['freq_eff_int'] = list(np.array(source_data['freq_eff']*1.0e-6).astype(int))\n",
    "\n",
    "        # Get the epochs of the source and convert them\n",
    "        # into isot and mjd formats\n",
    "        taustart = np.array(source_data['taustart_ts']).astype(str)\n",
    "        isot_times = astropy.time.Time(taustart, format='isot')\n",
    "\n",
    "        # Add columns to the source pandas table\n",
    "        # for the MJD, isot time, and frequency\n",
    "        source_data['mjd'] = isot_times.mjd\n",
    "        source_data['taustart_ts'] = taustart\n",
    "\n",
    "        # Set up a new pandas table so that you can\n",
    "        # sort the table the way you want to\n",
    "        mjds_ = []\n",
    "        isots_ = []\n",
    "        freqs_ = []\n",
    "        int_freqs_ = []\n",
    "        for im in image_values:\n",
    "            if ((source_data['freq_eff_int'] == int(im[0])) &\n",
    "                (source_data['mjd'] == im[1])).any():\n",
    "                pass\n",
    "            else:\n",
    "                new_row = dict()\n",
    "                for col in source_data.columns:\n",
    "                    new_row[col] = np.nan\n",
    "                    if col == 'mjd':\n",
    "                        new_row[col] = im[1]\n",
    "                    if col == 'freq_eff':\n",
    "                        new_row[col] = im[0]*1.0e6\n",
    "                    if col == 'freq_eff_int':\n",
    "                        new_row[col] = int(im[0])\n",
    "                    if col == 'taustart_ts':\n",
    "                        new_row[col] = astropy.time.Time(im[1], format='mjd').isot\n",
    "                source_data = source_data.append(new_row, ignore_index=True)\n",
    "\n",
    "        len_col = np.ones(len(source_data['ra']))\n",
    "        all_nans = len_col * np.nan\n",
    "\n",
    "        # Get the mean RA and DEC of the source\n",
    "        ras = np.array(source_data['ra'])\n",
    "        decs = np.array(source_data['decl'])\n",
    "        c = SkyCoord(ras*un.degree,\n",
    "                     decs*un.degree,\n",
    "                     frame='icrs')\n",
    "        # Find the distance to the phase centre and add it to the table\n",
    "        dist_to_phase_centre = phase_centre.separation(c)\n",
    "        source_data['dist_to_pc_DEG'] = dist_to_phase_centre.deg\n",
    "\n",
    "        # Add the runcat of the source to the table\n",
    "        source_data['runcat'] = (len_col*runcat).astype(int)\n",
    "        \n",
    "        # Sort the final table by MJD and frequency\n",
    "        source_data = source_data.sort_values(by=['freq_eff_int', 'mjd'])\n",
    "        source_data = source_data.drop_duplicates(subset=['mjd'])\n",
    "\n",
    "        # If you included the deep image at the\n",
    "        # start of the TraP run, remove\n",
    "        # that epoch completely and re-calculate\n",
    "        # the variability parameters\n",
    "        if drop_mjd == 'None':\n",
    "            # Make columns for the V and eta parameters\n",
    "            try:\n",
    "                source_data['V_param'] = Vs[r]\n",
    "            except TypeError:\n",
    "                print('V: ', Vs[r], type(Vs[r]))\n",
    "                source_data['V_param'] = np.nan\n",
    "\n",
    "            try:\n",
    "                source_data['eta_param'] = etas[r]\n",
    "            except TypeError:\n",
    "                print('eta: ', etas[r], type(etas[r]))\n",
    "                source_data['eta_param'] = np.nan\n",
    "\n",
    "            madval = madulation_parameter(np.array(source_data['f_int']))[0]\n",
    "            try:\n",
    "                source_data['mad_param'] = madval\n",
    "            except TypeError:\n",
    "                print('V: ', Vs[r], type(Vs[r]))\n",
    "                source_data['mad_param'] = np.nan\n",
    "        else:\n",
    "            drop_index = source_data.index[source_data['mjd']<drop_mjd][0]\n",
    "            source_data = source_data.drop(drop_index)\n",
    "\n",
    "            Vs = np.repeat(modulation_parameter(source_data[flux_col]),\n",
    "                           len(source_data[flux_col]))\n",
    "        \n",
    "            try:\n",
    "                etas = np.repeat(chi2_parameter(source_data[flux_col],\n",
    "                                                source_data['{}_err'.format(flux_col)]),\n",
    "                                 len(source_data[flux_col]))\n",
    "            except ZeroDivisionError:\n",
    "                print('source: ', r, 'etas error')\n",
    "                etas = np.repeat(np.nan, \n",
    "                                 len(source_data[flux_col]))\n",
    "\n",
    "            madps = np.repeat(madulation_parameter(source_data[flux_col])[0],\n",
    "                              len(source_data[flux_col]))\n",
    "\n",
    "            source_data['V_param'] = Vs\n",
    "            source_data['eta_param'] = etas\n",
    "            source_data['mad_param'] = madps\n",
    "        \n",
    "        source_lightcurves.append(source_data)\n",
    "\n",
    "    # Close your connection to the database\n",
    "    db._configured = False\n",
    "    del db, session\n",
    "    \n",
    "    return source_lightcurves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_duplicates(source_lightcurves, min_sep=3./60./60.):\n",
    "    '''\n",
    "    Sometimes TraP includes the same source twice.\n",
    "    This function removes duplicate sources based\n",
    "    on the right ascension and declination.\n",
    "    \n",
    "    TraP sometimes picks up the same source twice\n",
    "    usually recording two different light curves\n",
    "    for the same source where one light curve\n",
    "    includes few epochs than the other. Here we\n",
    "    check whether any sources are right on top\n",
    "    of each other using the RA and Dec, and remove\n",
    "    the source with fewer detections.\n",
    "    \n",
    "    Args:\n",
    "    source_lightcurves (str): List of pandas tables with the\n",
    "                              information for each source\n",
    "    kwargs:\n",
    "    min_sep (float): the minimum allowed separation\n",
    "                     between two sources in degrees\n",
    "                     Default: 3./60./60. degrees\n",
    "    Returns:\n",
    "    A list of pandas tables excluding duplicate\n",
    "    sources\n",
    "    \n",
    "    '''\n",
    "    # Get the coordinates of all of\n",
    "    # the sources\n",
    "    all_coords = np.zeros((len(source_lightcurves), 2))\n",
    "    for s, source in enumerate(source_lightcurves):\n",
    "        ra = np.nanmean(source['ra'])\n",
    "        dec = np.nanmean(source['decl'])\n",
    "        all_coords[s] = [ra, dec]\n",
    "    all_coords = SkyCoord(all_coords, unit=(un.degree, un.degree))\n",
    "\n",
    "    # Get the indices of sources that are\n",
    "    # too close together\n",
    "    delete_indices = []\n",
    "    for s, source in enumerate(source_lightcurves):\n",
    "        ra = np.nanmean(source['ra'])\n",
    "        dec = np.nanmean(source['decl'])\n",
    "        s_coord = SkyCoord(ra*un.degree, dec*un.degree)\n",
    "\n",
    "        seps = s_coord.separation(all_coords)\n",
    "        matches = np.where(seps.deg < min_sep)[0]\n",
    "\n",
    "        if len(matches) > 1:\n",
    "            flux_lengths = []\n",
    "            for m, match in enumerate(matches):\n",
    "                lc = source_lightcurves[m]\n",
    "                len_flux = np.where(~np.isnan(lc['f_int']))\n",
    "                flux_lengths.append(len(len_flux[0]))\n",
    "            delete_indices += list(matches[np.where(matches!=(matches[np.nanargmax(flux_lengths)]))[0]])\n",
    "    delete_indices = np.unique(np.array(delete_indices))\n",
    "\n",
    "    # Remove the sources that are duplicates\n",
    "    source_lightcurves_nodupes = []\n",
    "    for s, source in enumerate(source_lightcurves_nocut):\n",
    "        if s not in delete_indices:\n",
    "            source_lightcurves_nodupes.append(source)\n",
    "\n",
    "    print('Number of duplicates removed: {}'.format(len(delete_indices)))\n",
    "\n",
    "    return source_lightcurves_nodupes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sources(source_list,\n",
    "                 flux_col,\n",
    "                 database,\n",
    "                 dataset_id,\n",
    "                 savepath='/scratch/ldriessen/Source_Light_Curves/',\n",
    "                 extended_sources='GX339_KnownExtendedSources.npy'):\n",
    "    '''\n",
    "    Save each source as a CSV. Include whether the source\n",
    "    is an extended/resolved source\n",
    "    \n",
    "    Args:\n",
    "    source_list (str): List of pandas tables with the\n",
    "                       information for each source\n",
    "    flux_col (str): The name of the column in the source\n",
    "                    panda tables with the flux density in\n",
    "                    it\n",
    "    database (str): the name of the TraP database\n",
    "    dataset_id (int): the number of the dataset in the database\n",
    "    \n",
    "    Kwargs:\n",
    "    savepath (str): the name of the folder to save the\n",
    "                    light curve files\n",
    "    extended_sources (str): the name of the numpy file\n",
    "                            with the coordinates of the\n",
    "                            known extended sources\n",
    "    \n",
    "    Returns:\n",
    "    A list of updated pandas tables for each source\n",
    "    '''    \n",
    "    ext_s = np.load(extended_sources)\n",
    "    ext_s = SkyCoord(ext_s, unit=un.degree)\n",
    "    \n",
    "    scaled_sources = []\n",
    "    for s, source in enumerate(source_list):\n",
    "        # Get the frequency order for each pandas\n",
    "        # table to make sure you match everything\n",
    "        # up correctly by frequency\n",
    "\n",
    "        rcat = np.array(source['runcat'])[0]\n",
    "\n",
    "        # Check whether the source is\n",
    "        # a resolved/extended source\n",
    "        # and save accordingly\n",
    "        ra = (source['ra']).mean()\n",
    "        dec = (source['decl']).mean()\n",
    "        coords = SkyCoord(ra, dec, unit=un.degree)\n",
    "        seps = coords.separation(ext_s).deg\n",
    "        if np.nanmin(seps)*60.*60. < 3.:\n",
    "            savename = ('{0}rcat{1}_'\n",
    "                        'ra{2:.2f}_'\n",
    "                        'dec{3:.2f}_'\n",
    "                        'db{4}_ds{5}_'\n",
    "                        'ExtendedSource.csv').format(savepath,\n",
    "                                                     rcat,\n",
    "                                                     ra,\n",
    "                                                     dec,\n",
    "                                                     database,\n",
    "                                                     dataset_id)\n",
    "        else:\n",
    "            savename = ('{0}rcat{1}_'\n",
    "                        'ra{2:.2f}_'\n",
    "                        'dec{3:.2f}_'\n",
    "                        'db{4}_ds{5}_'\n",
    "                        'PointSource.csv').format(savepath,\n",
    "                                                  rcat,\n",
    "                                                  ra,\n",
    "                                                  dec,\n",
    "                                                  database,\n",
    "                                                  dataset_id)\n",
    "        source.to_csv(savename)\n",
    "        scaled_sources.append(source)\n",
    "\n",
    "    print('Done')\n",
    "\n",
    "    return scaled_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TraP database name\n",
    "database = 'Laura_NWLayers_ReProc'\n",
    "\n",
    "# The values needed to access TraP\n",
    "engine = 'postgresql'\n",
    "user =  'username' # your username here\n",
    "password = 'password'  # your password here\n",
    "host =  'vlo.science.uva.nl'\n",
    "port = 5432\n",
    "\n",
    "# Set up the path to where you want\n",
    "# to save the output files\n",
    "savepath = '/scratch/ldriessen/Source_Light_Curves/'\n",
    "# The file containing the ra and dec of\n",
    "# resolved sources and artefacts in\n",
    "# the field\n",
    "es_file = ('/scratch/ldriessen/'\n",
    "           '2021.01.18_GX339_KnownExtendedSources.npy')\n",
    "# The path to where you want to save the\n",
    "# image postage stamps\n",
    "im_savepath = '/scratch/ldriessen/NWLayers_Bands/'\n",
    "\n",
    "# The MJD of the deep image\n",
    "# that you are going to remove\n",
    "# from each light curve\n",
    "august_2018 = 58331\n",
    "\n",
    "# Define your phase centre and and the minimum number\n",
    "# of data point required in a light curve.\n",
    "phase_centre_ra = 255.706\n",
    "phase_centre_dec = -48.790\n",
    "min_dpts = 1\n",
    "\n",
    "# The dataset IDs of the datasets you want\n",
    "# to process\n",
    "dataset_ids = [49, 50, 51, 52, 53, 54, 55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d, dataset_id in enumerate(dataset_ids):    \n",
    "    print('All datasets: ', dataset_ids)\n",
    "    freq = freqs[d]\n",
    "\n",
    "    print('***********************************')\n",
    "    print('Dataset id: {}'.format(dataset_id))\n",
    "    print('Frequency: {} MHz'.format(freq))\n",
    "\n",
    "    # Get all the images that were run throuh\n",
    "    # TraP and get some basic information\n",
    "    images = glob.glob('/scratch/ldriessen/NWLayers/*{}*.fits'.format(freq))\n",
    "    images = np.array(sorted(images))\n",
    "\n",
    "    im_dates = []\n",
    "    for i, im in enumerate(images):\n",
    "        bn = im.split('/')[-1]\n",
    "        d = bn.split('_')[0]\n",
    "        im_dates.append(d)\n",
    "    im_dates = np.array(im_dates)\n",
    "    unique_images = np.unique(im_dates, return_index=True)\n",
    "\n",
    "    print('Total images: {}'.format(len(images)))\n",
    "    images = images[unique_images[1]]\n",
    "    print('Unique images: {}'.format(len(images)))\n",
    "\n",
    "    # Get the total number of epochs and their mjds,\n",
    "    # and get the full correct frequency/ies from the\n",
    "    # fits image headers\n",
    "    image_values = np.ones((len(images), 2))\n",
    "    for i, im in enumerate(images):\n",
    "        with fits.open(im) as f:\n",
    "            for hdu in f:\n",
    "                if hdu.header['CTYPE2'] == 'FREQ':\n",
    "                    freq = (hdu.header['CRVAL2'])*1e-6\n",
    "                elif hdu.header['CTYPE3'] == 'FREQ':\n",
    "                    freq = (hdu.header['CRVAL3'])*1e-6\n",
    "                elif hdu.header['CTYPE4'] == 'FREQ':\n",
    "                    freq = (hdu.header['CRVAL4'])*1e-6\n",
    "                obs_time = astropy.time.Time(hdu.header['DATE-OBS'])\n",
    "                obs_isot = obs_time.isot\n",
    "                obs_mjd = obs_time.mjd\n",
    "\n",
    "                image_values[i] = [freq, obs_mjd]\n",
    "    all_frequencies = np.sort(np.unique(image_values[:, 0]))\n",
    "    num_epochs = len(image_values)/len(np.unique(all_frequencies.astype(int)))\n",
    "    print('Frequencies observed: ', all_frequencies)\n",
    "    print('Number of images: {}'.format(len(images)))\n",
    "    print('Number of epochs: {}'.format(num_epochs))\n",
    "\n",
    "    # Get all of the light curves for this database and\n",
    "    # dataset. If you've included the deep image first,\n",
    "    # make sure to drop that mjd, this will remove that\n",
    "    # epoch completely\n",
    "    source_lightcurves_nocut = get_lightcurves(phase_centre_ra,\n",
    "                                               phase_centre_dec,\n",
    "                                               min_dpts,\n",
    "                                               engine,\n",
    "                                               host,\n",
    "                                               port,\n",
    "                                               user,\n",
    "                                               password,\n",
    "                                               database,\n",
    "                                               dataset_id,\n",
    "                                               image_values,\n",
    "                                               drop_mjd=august_2018)\n",
    "\n",
    "    print('Total number of sources: {}'.format(len(source_lightcurves_nocut)))\n",
    "\n",
    "    # Sometimes TraP grabs the same source twice. Get rid of any\n",
    "    # duplicates, removing the liht curve with few detections\n",
    "    new_source_list = delete_duplicates(source_lightcurves_nocut)\n",
    "\n",
    "    # Save each source as a csv\n",
    "    saved_sources = save_sources(new_source_list,\n",
    "                                 'f_int',\n",
    "                                 database,\n",
    "                                 dataset_id,\n",
    "                                 savepath=savepath,\n",
    "                                 extended_sources=es_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
